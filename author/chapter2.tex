%%%%%%%%%%%%%%%%%%%%% chapter2.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  Monotone Circuits Lower bounds 
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%
%\motto{Use the template \emph{chapter.tex} to style the various elements of your chapter content.}





\chapter{Monotone Circuit Lower Bounds}
\label{sec:Razborov} % Always give a unique label
% use \chaptermark{}
% to alter or adjust the chapter heading in the running head


% Always give a unique label
% and use \ref{<label>} for cross-references
% and \cite{<label>} for bibliographic references
% use \sectionmark{}
% to alter or adjust the section heading in the running head

We have  seen that proving that SAT is not in \Ppoly, i.e., cannot  be solved by polynomial-size circuits, implies that $\P \neq \NP$.
Due to the notorious difficulty of this and related questions, we are also interested in proving \emph{weaker} lower bounds, namely, lower bounds against \emph{restricted} classes of circuits. Although this does not settle the main lower bound questions, it is still considered an important step towards the bigger questions, at least from the methodological perspective. 
Here, we study such a restricted circuit class and prove a lower bound against it: Boolean circuits without negation gates, which are also called \emph{ monotone circuits}.

\para{Monotone Circuits}

\begin{definition}[Monotone circuit]
A \emph{monotone circuit} is a Boolean circuit that contains fan-in two gates AND and OR, but has \emph{no} NOT gates.
\end{definition}

Note in particular that monotone circuits can compute only monotone functions: a Boolean function is said to be monotone if \emph{increasing the number of ones} in the input cannot flip the value of the function from 1 to 0. More precisely, for $\bar{x}, \bar{y} \in\{0,1\}^n$, write $\bar{x} \geq \bar{y}$ iff $ \forall i \in [n], x_i \geq y_i$, where $[n]$ denotes $\{1,\dots,n\}$. (Here, $x_i\ge y_i$ for Boolean $x_i,y_i$ means simply that $1\ge 0$ and $0\ge 0$, $1\ge 1$, while $0\not\ge 1$.)

\begin{definition}[Monotone function]
A Boolean function $f:\{0,1\}^n \rightarrow\{0,1\}$ is said to be  \emph{monotone} if $\forall \bar{x} \geq \bar{y}, f(\bar{x}) \geq f(y)$.
\end{definition}

\section{The CLIQUE Problem}
Many \NP-complete problems are monotone. One example of an \NP-complete decision problem is  CLIQUE we describe now. 
Given an undirected graph $G=(V, E)$ with $n$ nodes, a \emph{$k$-clique} in $G$ is a set $U\subseteq V$ of size $k$, such that  every pair of nodes $u_1, u_2 \in U$ is connected by an edge (in $E$), and in symbols:
$$
 \forall u_1 \in U~\forall u_2 \in U~ ( u_1 \neq u_2\Rightarrow (u_1, u_2)\in E).
$$


Recall that a computational decision problem is a language over a finite alphabet (usually \bits). Here, our language consists of all the strings that encode (in some natural way) an accepted graph, i.e., a graph with $n$ nodes that contains a $k$-clique.
The natural way to encode a graph in our case is this: a graph  $G=(V, E) $ with $n$ nodes, is encoded by $\binom{n}{2}$ input variables  $x_{ij}$, where the semantic of the encoding is: $x_{i j}=1$ iff $(i, j) \in E$. In other words, if the input variable $x_{ij}=1$,   our input graph contains the edge $(i,j)$, and otherwise it does not. 

We are interested in CLIQUE$(k, n)$ for a \emph{fixed} $k$, considered as the following Boolean function: 
\begin{svgraybox}
The computational problem \textbf{CLIQUE$(k, n)$}: 

\textit{Input}: Undirected graph $G=(V,E)$ with $n$ nodes encoded as a length $n\choose 2$ binary string (each bit represents an  edge  $x_{ij}$, $i<j\in[n]$), and a number $k$ (given in unary, i.e., $1^k$).

\textit{Accept}: if the graph $G$ contains a $k$-clique. 

\textit{Reject}: otherwise.
\end{svgraybox}


 
 
It is known that CLIQUE$(k,n)$ is NP-complete (see standard complexity textbooks; e.g., Papadimitriou 1994).
Note that $\operatorname{CLIQUE}(k, n)$ is a monotone function: if we add 1's to the input, we only \emph{increase} the chance it has a $k$-clique. Since CLIQUE$(n, k)$ is a monotone (Boolean)
function we can compute it by a monotone Boolean circuit. But the question remains whether we can compute CLIQUE$(n, k)$ with small monotone circuit. 

\begin{trailer}{Example of a monotone circuit computing CLIQUE$(n, k)$}
Consider  all $\binom{n}{k} ~~ k$-sub-graphs in $G$, and check if at least one of those is a clique:

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{images/k-clique-simple-circuit.png}
    \label{fig:clique-naive}
    \caption{Naive way to compute CLIQUE}
\end{figure}

$S_1, S_2, \ldots, S_{\binom{n}{k}}$ are the $\binom{n}{k}$ subgraphs in $G$ each of size $k$.
Size of this circuit: $O\left(k^2 \cdot\binom{n}{k}\right)$.
\end{trailer}

\subsection{Approximators}

A  circuit for computing $\operatorname{CLIQUE}  (n, k),$ consisting of an OR gate $\bigvee$ of many  $S_i$'s, each of size $k$ and where each $S_i$ is an AND of the edge variables in $S_i$, as in the example above, is called an \textit{approximator} for CLIQUE($n, k)$. More formally, we have the following. 

% *Do not* use the \begin{tcolorbox}[float=..] %option, unless you wish it to float around; namely %not appear sequentially as you put in in the %text. 

\begin{tcolorbox}[colframe=white, colback=red!5, boxrule=0mm, sharp corners]
\textbf{Approximators $CC\left(X_1, \ldots, X_m\right)$ for CLIQUE}.
Let $V$ be a set of nodes and let $X_1,\dots,X_n\in V$ be a collection of subsets of nodes. The \emph{approximator} $CC\left(X_1, \ldots, X_m\right)$ is defined to be 
$\bigvee_{r=1}^m\bigwedge_{i<j \in X_r} x_{ij}$.
Note that the $X_i$'s may have different sizes (and specifically their sizes may be different from $k$).
\end{tcolorbox}
 
Note that $CC\left(X_1, \ldots, X_m\right)(\alpha)=1$, for $\alpha\in\bits^{n\choose 2}$, precisely when in the graph $G$ over the nodes $V$ described by the assignment $\alpha$, at least one of the $X_i$ subgraphs is a clique. 

We shall also use the abbreviated notation $CC(\mathcal X)$ for $CC(X_1,\dots,X_m)$ when $\mathcal X = \{X_1,\dots,X_m\}$.

% We denote by $CC\left(S_1, \ldots, S_{\binom{n}{k}}\right)$ the approximator computing the $\bigvee$ of all subgraphs $S_1, \ldots, S_{n\choose k }$.

\begin{trailer}{Example of simple useful asymptotic computations}
%\begin{note} 
Note that when $k\in(\Omega(\log n),O(n-\log n))$, $CC\left(S_1, \ldots, S_{n\choose k}\right)$, where $S_1,\dots,S_{n\choose k}$ are all possible subsets of size $k$ from the set of $n$ nodes $V$, is of \emph{super-polynomial size}, because $ n\choose k$ is super polynomial for $k$ in this range. For the sake of getting used to asymptotic estimates such as these, it is helpful to go over the computations in more detail, as follows.

We shall estimate the asymptotic behaviour of $\binom{n}{\log n} $, showing it is superpolynomial, namely, $\binom{n}{\log n} =
n^{\Omega(\log n)}$. First, we observe that

$$\binom{n}{k} \ge\left(\frac{n}{k}\right)^k. $$
To prove this lower bound we do the following.
Write $\binom{n}{k}=\frac{n!}{k!(n-k)!}=
\frac{\prod_{j=0}^{k-1}(n-j)}{k!}=\prod_{j=0}^{k-1} \frac{n-j}{k-j}$. Notice that each factor $\frac{n-j}{k-j}$ in this product is at least $\frac{n}{k}$, and there are $k$ factors, giving the lower bound $\left(\frac{n}{k}\right)^k$. 

Now, to lower bound $\binom{n}{\log n} $ we have:

\[  
\begin{aligned}
 \binom{n}{\log n} \geq\left(\frac{n}{\log n}\right)^{\log n}
& =
\left(
    \frac{2^{\log n}}
        {2^{\log (\log n)}}\right)^{\log n} \\
& =2^{(\log n-\log \log n) \cdot \log n} \\
& =2^{\log^2n-\log n \cdot \log \log n} \\
& =2^{\log ^2 n-o(\log n)} \\
& = 2^{c \log ^2 n} \text {, ~~~for some constant $c$, say, $c=1/2$} \\
& =\left(2^{\log n}\right)^{c \cdot \log n}=
n^{c \log n},
\end{aligned}
\]
which means that $\binom{n}{\log n}= n^{\Omega(\log n)}$.



% ------------------ Using Stirling

% Stirling's approximation for factorials helps simplify the expressions: 
% \begin{tcolorbox}[colframe=white, colback=gray!5, boxrule=0mm, sharp corners]
% \textbf{Stirling's Approximation}:
% $n!\sim \sqrt{2 \pi n}\left(\frac{n}{e}\right)^n$, where $e$ is the natural logarithm.
% \end{tcolorbox}
% 
% By definition, we have $\binom{n}{\log n} =\frac{n!}{\log n!(n-\log n)!}$. Hence, using Stirling's approximation to substitute the factorials we get:
% \[
%  \binom{n}{\log(n)} \sim \frac{\sqrt{2 \pi n} \left(\frac{n}{e}\right)^n}{\sqrt{2 \pi \log(n)} \left(\frac{\log(n)}{e}\right)^{\log(n)} \sqrt{2 \pi (n - \log(n))} \left(\frac{n - \log(n)}{e}\right)^{n - \log(n)}}.
% \]
%  
% Simplifying, we combine the square root terms:
%    \[
%    \frac{\sqrt{2 \pi n}}{\sqrt{2 \pi \log(n)} \sqrt{2 \pi (n - \log(n))}} \sim \frac{1}{\sqrt{2 \pi \log(n)}}.
%    \]
% Simplify the exponentials:    the \(\left(\frac{n}{e}\right)^n\) term dominates the numerator, and dividing by \(\left(\frac{\log(n)}{e}\right)^{\log(n)}\) and \(\left(\frac{n - \log(n)}{e}\right)^{n - \log(n)}\) leads to:
% 
%    \[
%    \left(\frac{n}{n - \log(n)}\right)^{n - \log(n)} \cdot \left(\frac{n}{\log(n)}\right)^{\log(n)}.
%    \]
% 
% For large \(n\), \(n - \log(n) \sim n\), namely, $n-\log (n) =\Omega(n)$, so:
%  $
%    \left(\frac{n}{n - \log(n)}\right)^{n - \log(n)} \sim 1.
%  $
% Hence, the dominant term becomes $\left(\frac{n}{\log(n)}\right)^{\log(n)}.$
% Thus 
% \[
% \binom{n}{\log(n)} \sim \frac{1}{\sqrt{2 \pi \log(n)}} 
% \cdot \left(\frac{n}{\log(n)}\right)^{\log(n)}
% \ge  \left(\frac{n}{\log(n)}\right)^{c\log(n)}, 
% \]
% ======
%=========================================== 
% % for some constant $c$, and all sufficiently large $n$'s.
% % This is the same as writing
% %\[
% %\binom{n}{\log(n)} 
% %=  \left(\frac{n}{\log(n)}\right)^{\Omega(\log(n))}. 
% %\]
% %
% Using the fact that $2^{\log(\log n)}=\log n$,  we get:
% \begin{align*}
% \binom{n}{\log n} & \ge 2^{c\cd\log n\cd(\log n-\log(\log n))}\\
% &   = 
% 2^{c\cd\log n\cd\log n-c\cd\log n \cd\log(\log n))}\\
% &   = 
% 2^{c\cd\log^2 n-o(\log ^2n)}\\
% &   =
% 2^{\Omega(\log^2(n))} = n^{\Omega(\log n)}.
% \end{align*}

\end{trailer}
% 
% To asymptotically estimate \(\binom{n}{\log(n)}\), we can use the binomial coefficient formula and apply approximations for large \(n\). The binomial coefficient is defined as:
% 
% \[
% \binom{n}{k} = \frac{n!}{k!(n-k)!}.
% \]
% 
% Here, \(k = \log(n)\), where we assume the logarithm is base 2 unless specified otherwise. For large \(n\), this becomes:
% 
% \[
% \binom{n}{\log(n)} = \frac{n!}{(\log(n))!(n - \log(n))!}.
% \]
% 
% ### Using Stirling's Approximation
% Stirling's approximation for factorials (\(n! \sim \sqrt{2 \pi n} \left(\frac{n}{e}\right)^n\)) helps simplify the expressions. Applying this:
% 
% 1. For \(n!\):
%    \[
%    n! \sim \sqrt{2 \pi n} \left(\frac{n}{e}\right)^n.
%    \]
% 
% 2. For \((\log(n))!\):
%    \[
%    (\log(n))! \sim \sqrt{2 \pi \log(n)} \left(\frac{\log(n)}{e}\right)^{\log(n)}.
%    \]
% 
% 3. For \((n - \log(n))!\):
%    Since \(n - \log(n) \sim n\) for large \(n\), we approximate:
%    \[
%    (n - \log(n))! \sim \sqrt{2 \pi (n - \log(n))} \left(\frac{n - \log(n)}{e}\right)^{n - \log(n)}.
%    \]
% 
% ### Substituting
% Now substitute these approximations into the binomial coefficient:
% 
% \[
% \binom{n}{\log(n)} \sim \frac{\sqrt{2 \pi n} \left(\frac{n}{e}\right)^n}{\sqrt{2 \pi \log(n)} \left(\frac{\log(n)}{e}\right)^{\log(n)} \sqrt{2 \pi (n - \log(n))} \left(\frac{n - \log(n)}{e}\right)^{n - \log(n)}}.
% \]
% 
% ### Simplifying
% 1. Combine the square root terms:
%    \[
%    \frac{\sqrt{2 \pi n}}{\sqrt{2 \pi \log(n)} \sqrt{2 \pi (n - \log(n))}} \sim \frac{1}{\sqrt{2 \pi \log(n)}}.
%    \]
% 
% 2. Simplify the exponentials:
%    The \(\left(\frac{n}{e}\right)^n\) term dominates the numerator, and dividing by \(\left(\frac{\log(n)}{e}\right)^{\log(n)}\) and \(\left(\frac{n - \log(n)}{e}\right)^{n - \log(n)}\) leads to:
% 
%    \[
%    \left(\frac{n}{n - \log(n)}\right)^{n - \log(n)} \cdot \left(\frac{n}{\log(n)}\right)^{\log(n)}.
%    \]
% 
%    For large \(n\), \(n - \log(n) \sim n\), so:
%    \[
%    \left(\frac{n}{n - \log(n)}\right)^{n - \log(n)} \sim 1.
%    \]
% 
%    The dominant term becomes:
%    \[
%    \left(\frac{n}{\log(n)}\right)^{\log(n)}.
%    \]
% 
% ### Final Asymptotic Estimate
% Thus, the leading-order asymptotic behavior of \(\binom{n}{\log(n)}\) is:
% 
% \[
% \binom{n}{\log(n)} \sim \frac{1}{\sqrt{2 \pi \log(n)}} \cdot \left(\frac{n}{\log(n)}\right)^{\log(n)}.
% \]
% 
% This grows extremely rapidly as \(n\) increases.

% ===========================
% end Stirling computation 
% 

\section{Clique is hard for monotone circuits }
The following theorem shows the naive (brute-force) way of computing the CLIQUE function with a monotone circuit shown in Figure \ref{fig:clique-naive}    cannot be improved much.

\begin{figure}
    \centering
    \includegraphics[width=0.3\linewidth]{images/RAZBOROV_Alexander.jpeg}
    \caption{Alexander Razborov. Creator: Kozloff, Robert | Credit: Photo by Robert Kozloff.
Copyright: The University of Chicago}
    \label{fig:enter-label}
\end{figure}


\begin{tcolorbox}[colframe=white, colback=blue!5, boxrule=0mm, sharp corners]
\begin{theorem}[Razborov \cite{Razb85}]\label{thm:razborov} Let $k=\sqrt[4]{n}$. Then, every monotone circuit computing {\rm CLIQUE}$(n, k)$ has size $2^{\Omega(\sqrt[8]{n})}$.
\end{theorem}
\end{tcolorbox}
That is, there exists a constant $c$ such that  for large enough $n \in \mathbb{N}$, if $C_n$ computes CLIQUE$(n,k)$ then $\left|C_n\right| \geq 2^{c \cdot \sqrt[8]{n}}$.


The rest of this chapter aims to prove this theorem. \textit{Our exposition is taken from Papadimitriou's textbook} \cite{Pap94}.


%Recall: a approximator is a big OR of cliques, each computed as a big AND.




\section{Proof of monotone circuit lower bounds \Cref{thm:razborov} using the Approximation Method}


\paragraph{Approximation Method Overview}

We first provide an overview of the approach we take to prove \Cref{thm:razborov} which is called \emph{the approximation method}. 
We shall describe a way of approximating any \textbf{monotone} circuit for $\operatorname{CLIQUE}({n}, {k})$ by a approximator, namely a big OR of cliques, as follows.
 

\begin{tcolorbox}[colframe=white, colback=blue!5, boxrule=0mm, sharp corners]
\textbf{Approximation Method}:
The upshot of the method is the following: \emph{by  way of contradiction,} consider a purported small monotone circuit computing  $\operatorname{CLIQUE}({n}, {k})$, and show by induction, gradually progressing from the input nodes towards the output node, that the function computed at every gate can be well approximated by a small approximator. Then, if the number of nodes in the circuit is small the output gate is also well approximated by a small approximator. Now, use an auxiliary argument to show that there is no small approximator approximating well  $\operatorname{CLIQUE}({n}, {k})$.
\end{tcolorbox}
% =======
% The upshot is the following: \emph{by way of contradiction,} consider a purported small monotone circuit computing  $\operatorname{CLIQUE}({n}, {k})$, and show by induction, gradually progressing from the input nodes towards the output node, that the function computed at every gate can be approximated by a small approximator. Then, the output gate is also approximated by a small approximator. Now, use an auxiliary argument to show that there is no small approximator approximating $\operatorname{CLIQUE}({n}, {k})$.
% >>>>>>> f39d2195a6351cafba5750ac78532621d361fb0c

More precisely, we have:  
\begin{enumerate}
 
\item  Given a monotone circuit $C$, we shall construct a approximator 
$CC(X_1, \ldots, X_m)$ for some $m$ and $|{X}_{i}| \leq l$ (for some $l$, and for $i=1, \ldots, m$ ), that approximates $\operatorname{CLIQUE}({n}, {k})$ with \textbf{precision} that is dependent on the number of gates in $C$.
% =======
% \item  Given a monotone circuit $C$, we shall construct a approximator ${CC}({X_1, \ldots, {X_m})}$ for some $m$ and $\left|{X}_{{i}}\right| \leq l$ (for some $l$, and for ${i}=1, \ldots, {m}$ ), that approximates $\operatorname{CLIQUE}({n}, {k})$ with \textbf{precision} that is dependent on the number of gates in $C$.
% >>>>>>> f39d2195a6351cafba5750ac78532621d361fb0c

\item That is, if the number of gates in $C$ is small the precision is  good, namely the approximator ${CC}(X_1, \ldots, X_m)$ for $\operatorname{CLIQUE}({n}, {k})$ we end up with makes \textit{few} mistakes on the $\operatorname{CLIQUE}({n}, {k})$ function (a mistake happens when the circuit  answers  "NO" on an input that has a $k$-clique, or "YES" on an input that has no $k$-clique).\label{it:approximation-b}

The \textbf{approximation} (i.e., construction of an approximator for CLIQUE(${n},{k})$ given the circuit $C$) will proceed in steps, one step for each gate of the monotone circuit:

\begin{enumerate}
    
\item If $C$ is a monotone circuit computing $\operatorname{CLIQUE}({n}, {k})$ we can \textit{approximate} any gate OR or AND in $C$ with an approximator.

\item Each such approximation step introduces rather few errors (false positives and false negatives).
\end{enumerate}



\item We show that every approximator ${CC}({X}_1, \ldots, {X_m})$ ($|{X}_i| \leq l$ for some $l$, for all $i=1, \ldots, m$ ), ought to make \textit{exponentially many errors} on the function $\operatorname{CLIQUE}(n, k)$. From \ref{it:approximation-b} above we conclude that there exists no circuit $C$ with a small number of gates. 
\end{enumerate}





\begin{trailer}{Parameters \& notation}

Recall we want to compute CLIQUE$(n, k)$
with $n$ the number of nodes in the graph and $k$ the size of a clique within the graph. 
We set:
$$
k=\sqrt[4]{n}.
$$


$$
\begin{aligned}
& l=\sqrt[8]{n} \\
& p \approx \sqrt[8]{n} \\
& M=(p-1)^l \cdot l! & \approx(\sqrt[8]{n}-1)^{\sqrt[8]{x}} \cdot(\sqrt[8]{n})! \\
& & \leq(\sqrt[4]{n})^{\sqrt[8]{n}}
\end{aligned}
$$


Each (hypothetical) approximator we use in the approximation is:

$$
C C\left(X_1, \ldots, X_m\right)
$$

for $m \leq M$ and $\left|X_i\right| \leq l, \forall i \in[m]$.
\end{trailer}

\newcommand{\cliquenk}{\ensuremath{\operatorname{CLIQUE}(n,k)}}

%___________________________________________________________________
\subsection{The Input Set: Extreme Graphs, False Positive and  Negatives}
%___________________________________________________________________

Here we consider the input graphs we are going to analyse. 
The input set to the \cliquenk\ function are \emph{all possible} binary strings of length $n\choose 2$, namely, all possible (encodings) of undirected graphs with $n$ nodes. This set is easily divided into two groups:

\begin{description}
\item[Accept-instances:] 
$G=(V, E)$ is a graph on $n$ modes that contains a $k$-clique.
\item[Reject-instances:]
$G=(V, E)$ is a graph on $n$ nodes that does not contain a $k$-clique.
\end{description}


 



A crucial point is that \emph{we are going to restrict attention only to a special \textbf{subset} of all possible input graphs}. We call these special input graphs \emph{extreme graphs}. 

\para{Extreme Graphs}
We  restrict attention to only a subsets of accept and reject instances. This is sufficient for the proof (and simplifies it). 
Because extreme graphs are part of the input set, a correct circuit clearly must answer correctly on this subset. So, if we show that there must be mistakes made by small circuits on this subset of input graphs, it is immediate that there is no small monotone circuit that correctly computes \cliquenk.



Thus, our focus is on ``extreme'' cases of inputs:

\begin{description}
\item[\textbf{Positive-inputs}:] 
 $G=(V,E)$ has $n$ nodes and a $k$-clique (i.e., a set of $k$ nodes with all edges between them); while \emph{no} other edge exist in $G$ except for the $k$-clique.

There are $\binom{x}{k}$ positive-inputs, each one is clearly an accept input to \cliquenk.
For simplicity, we shall call these positive inputs \emph{$k$-\textbf{cliques}} (although these are special extreme cases of $k$-cliques, since we assume that all edges outside the clique are absent.) 

\begin{figure}[H]
    \centering
    \includegraphics[width=.5\linewidth]{images/clique1.png}
    \caption{Example of a \emph{positive input}: a graph whose only edges are part of a (single) $k$-clique.}
    \label{fig:enter-label}
\end{figure}


\item[\textbf{Negative-inputs}:] 

$G=\left(V,E\right)$ has $n$ nodes and a $(k-1)$-colouring. I.e., $k-1$ independent sets (each with its own distinct colour), namely sets of nodes, such that in each set there are no edges between nodes. Moreover,  \emph{all} edges between nodes in  different independent sets are present!
For simplicity, we shall call these negative inputs \emph{$(k-1)$-\textbf{colouring}} (although these are special extreme cases of $k-1$-colourable graphs, since we assume that all edges between independent sets are present). 


\begin{figure}[H]
    \centering
    \includegraphics[width=.6\linewidth]{images/clique2.png}
    \caption{Example of a \emph{negative input} (i.e., $k-1$-colouring): a graph whose all and only edges are between distinct $(k-1)$-independent sets.}
    \label{fig:enter-label}
\end{figure}

\end{description}

\begin{note}\

\begin{enumerate}
\item There are $(k-1)^n$ negative -inputs. (We count twice two identical graphs with colours interchanged.

\item 
 A $(k-1)$-colouring is a negative-input for \cliquenk\ because a $k$-clique cannot be coloured by $k-1$ colours.
\item
 In fact, even a single edge added to a $(k-1)$-colouring will make the graph contain a k-clique!
\end{enumerate}
\end{note}

\para{False Positive and False Negative Inputs}

Let $V$ be a set of $n$ nodes, and  $CC(\mathcal X)$ be an   approximator, with  $\mathcal X$ a collection $X_1\dots,X_m\subseteq V$ of subsets of nodes. We say that  a graph $G(V,E)$ (with $n$ nodes) is a \textbf{false positive} if $CC(\mathcal X)(G)=1$ while $G$ is  a $(k-1)$-colouring (and hence, not a $k$-clique). Similarly, a graph $G$ with $n$ nodes $V$ is said to be a \textbf{false negative} if $CC(\mathcal X)(G)=0$ while $G$ \emph{is} a $k$-clique.


%___________________________________________________________________
%\subsection{Approximating $\lor,\land$ Gates with approximators %}
%___________________________________________________________________





%\begin{tcolorbox}[colframe=white, colback=red!4, boxrule=0mm, sharp %corners]
%\begin{lemma}[Approximating ]
%\label{lem:}
%
%\end{lemma}
%\end{tcolorbox}


%___________________________________________________________________
\subsection{Concluding the Lower Bound using Structural Induction    }
%___________________________________________________________________



% Recall that our goal is:
% 
% \begin{tcolorbox}[colframe=white, colback=blue!1, boxrule=0mm, sharp corners]
% \textbf{Goal}: Show that every monotone circuit computing $\operatorname{CLIQUE}(n,k)$ has size at least $2^{c \sqrt[8]{n}}$ for some constant $c$ (for sufficiently large $n$).
% \end{tcolorbox}



What does it mean that an approximator C(X) \emph{approximates} the monotone circuit C? It means that ``on most'' extreme input graphs $G$, $C(G) = CC(X)(G)$, namely the approximator agrees with the actual circuit on most $G$'s. 

The approximation of the  monotone circuit $C$ that computes $\operatorname{CLIQUE}(n, k)$ is constructed  by\emph{ induction on the size of $C$,} i.e., number of $\lor, \wedge$ gates in $C$.


\begin{tcolorbox}[colframe=white, colback=gray!11, boxrule=0mm, sharp corners]
\begin{note} Such an induction is usually called \emph{\textbf{induction on the structure of the circuit} $C$} (or \emph{structural induction}), since we are going to induce on subcircuits of $C$, assuming the induction hypothesis holds for them. Using the structure of the circuit, we shall conclude that the induction hypothesis  holds for the gate that connects those two subcircuits. The base case is showing that the induction statement holds for circuits of size 1 (i.e., singled variable circuits). The induction step is showing that the induction statement holds for $A\lor B$ assuming the induction statement holds for both $A$ and $B$. And similarly for the case of $A\land B$.     
\end{note}
\end{tcolorbox}

 

Now, assume that $C$ is a monotone circuit we wish to build an approximation for. We are going to build the approximator for the output gate $C$ gradually, from the input gates of $C$  through the internal gates, up to the output gate, so that the approximator of the output gate will be the approximator of the circuit $C$. The skeleton of this construction is done as follows, following a standard structural induction scheme:

\begin{trailer}
{Approximation of the output gate of $C$ via structural induction}

\begin{description}
\item[\textbf{Base step}:] 
Input nodes $x_{ij}$. Build an approximator $CC(x_{ij})$ (we shall see this in the next section).

\item[\textbf{Induction step (the Approximator Step)}:]
\

\case 1 OR gate $g_1\lor g_2$. By induction hypothesis we already built the approximators of $g_1,g_2$, $CC(g_1),CC(g_2)$, respectively. We thus need to construct $CC(g_1\lor g_2)$ from $CC(g_1),CC(g_2)$ (we shall do this in the sequel).

\case 2 AND gate $g_1\land g_2$. 
By induction hypothesis we already built the approximators 
of $g_1,g_2$, $CC(g_1),CC(g_2)$, respectively. 
We thus need to construct $CC(g_1\land g_2)$ from 
$CC(g_1),CC(g_2)$ (again, we shall do this in the sequel).
\end{description} 
\end{trailer}

 We call the two cases of the induction step above \emph{the approximator steps}. Namely, we have an approximator step for an OR gate and an AND gate, respectively. 


\begin{definition}[Introducing new false positives and false negative]
Let the graph $G$ be a false positive of $CC(g_1\lor g_2)$ (i.e., a $(k-1)$-colouring of which $CC(g_1\lor g_2)(G)=1$).
We say the approximator step \emph{\textbf{introduced $G$ as a new false positive}} if $(CC(g_1)\lor CC(g_2))(G)=0$, namely, $G$ is \emph{not} a false positive of $(CC(g_1)\lor CC(g_2))$.
Similarly, let the graph $G$ be a false positive of $CC(g_1\land g_2)$. We say the approximator step \textbf{\emph{introduced $G$ as a new false positive}} if $(CC(g_1)\land CC(g_2))(G)=0$.
\end{definition}

To prove the lower bound theorem, all we need to know are some properties of the approximator steps. In particular, the following two lemmas, proved in the next section, are sufficient to conclude the proof.   







\begin{lemma}\label{lem:new-false-positives}
Each approximation step introduces at most $M^2 \cdot \frac{(k-1)^n}{2^p}$ false positives.
\end{lemma}

\begin{lemma}\label{lem:new-false-negatives}
Each approximation step introduces at most $M^2 \cdot\binom{n-l-1}{k-l-1}$ false negatives.
\end{lemma}


\bigskip









\begin{proof}[Proof of \Cref{thm:razborov}   from \Cref{lem:new-false-positives} and \Cref{lem:new-false-negatives}]
\

We use the following claim. 

\begin{claim}\label{cla:}
 Every approximator  CC$\left(X_1, \ldots, X_m\right)$ with 
  $\left|X_i\right| \leq l$ and $m \leq M$ is either identically 0 (and thus is wrong on all positive instances, namely all positive instances are false negatives in this case), or outputs 1 on at least half of the (extreme) negative instances (namely, at least half of the negative instances are false positives).
\end{claim}
\begin{proof}
If $\operatorname{CC}\left(X_1 \ldots X_m\right)$ is not identically 0 (namely, not the OR of \emph{zero} ANDs, i.e., not the formula $0$), then it accepts at least those graphs $G$ that have cliques on at least one of the sets $X_i$, for some $i\in[m]$. The size of $X_i$ is at most $ l<k$, thus many graphs $G$ that are not $k$-cliques still have $l$-cliques (because the approximator evaluates to 1 when there is a $|X_i|$-clique on the nodes in $X_i$).

Let us  estimate  how many such extreme graphs exist. 
Namely, we wish to show that there are many negative instances, i.e., $k-1$-colourings, that will nevertheless satisfy  
$\operatorname{CC}\left(X_1 \ldots X_m\right)$.
It is simple to count this  using \emph{probability}. Consider a graph $G=(V, E)$ and  assign randomly (and independently) from among $k-1$ colours to the nodes of $G$. Let $v_1 \neq v_2 \in V$ be two nodes, then we  have
$$
\operatorname{Pr}\left[\text { colour of } v_1=\text { colour of } v_2\right]=\frac{1}{k-1},
$$
because regardless of the colour $v_1$ got, there is only one colour out of $k-1$ options for $v_2$ that will match the colour of $v_1$.

Denote by $R\left(X_i\right)$ the event that in the random colouring of $G$, there exists a pair of nodes with the same colour in $X_i$.
Then, 
$$
\operatorname{Pr}\left[R\left(X_i\right)\right] \leq \frac{\binom{|X_i |}{2}}{k-1} \leq \frac{\binom{l}{2}}{k-1} \leq \frac{1}{2}.
$$
The first inequality from the left holds because there are $\binom{\mid X_i |}{2}$ pairs
of nodes in $X_i$, and 
the union bound.
The rightmost inequality holds because 
$ \frac{\binom{\sqrt[8]{n}}{2}}{\sqrt[4]{n}-1}
= \frac{\frac{\sqrt[8]{n} \cdot(\sqrt[8]{n}-1)}{2}}{\sqrt[4]{n}-1} \le \frac{\sqrt[4]{n}}{2\sqrt[4]{n}}=\frac{1}{2}.
$



This means that out of all negative instances to CLIQUE $(n, k)$ (namely, all $(k-1)$-colourings with $n$ nodes), at least half of them are going to colour $X_i$ with \emph{different} colours. Hence, for these negative instances, there will be edges between all nodes in $X_i$, and thus for each of these negative instances $G$ we have $CC\left(X_1, \ldots, X_m\right)(G)=1$.
\end{proof}




\begin{tcolorbox}[colframe=white, colback=green!4, boxrule=0mm, sharp corners]
\textbf{Union Bound}: if in a random experiment
there are $m$ possible ``bad'' events, each with probability at most $\alpha$, then the probability that at least one of the ``bad'' events occurred is at most $m\alpha$.

\small
\medskip 
---

We recall some  basic notions from  discrete probability, and explain more formally the union bound in what follows.

In discrete probability we consider a \emph{ Sample Space}
$\Omega$ as a finite set of  all possible outcomes of a random experiment. The experiment is random in the sense that its outcomes are not deterministic rather hold  probabilities. 
Each element $a$ in $\Omega$ is a possible outcome of the  random experiment, and is sometime called an \emph{atomic outcome}, and  holds a precise probability denoted $\Pr\left[a \right]$, such that:
\begin{enumerate}
\item $0\le \Pr\left[a\right]\le 1$, for all $a\in\Omega$.

\item $\Sigma_{a\in\Omega}\Pr[a]=1$.
\end{enumerate} 

An \emph{event} is a set $E\subseteq\Omega$ of atomic outcomes. The probability of an event $E\subseteq\Omega$ is naturally defined as $\Sigma_{a\in E}\Pr[a]$. 


\bigskip 
The \emph{union bound} provides an upper bound on the probability of the union of multiple events. Formally, if \( E_1, E_2, \dots, E_n \) are events in a probability space, the union bound states:

\[
P\left(\bigcup_{i=1}^n E_i\right) \leq \sum_{i=1}^n P(E_i)
\]

Intuitively, the union bound tells us that the probability of at least one of the events \( E_1, E_2, \dots, E_n \) occurring is at most the sum of the probabilities of each individual event. This bound is useful because it is often \emph{much easier} to calculate or estimate the probabilities of \emph{individual} events than to calculate the exact probability of their union.

 If some events are not disjoint, their combined probability is generally less than the sum of their probabilities, so the union bound is not tight in this case. The union bound is tight (exact) if and only if the events are disjoint. Otherwise, it overestimates the probability due to double-counting overlaps among the events.




\end{tcolorbox}

We are now ready to conclude the proof of \Cref{thm:razborov} (given the two lemmas \ref{lem:new-false-positives} and \ref{lem:new-false-negatives}).

Recall: $l=\sqrt[8]{n}$ and
let $p=\sqrt[8]{n} \cdot \log n$. 
Thus, $M=(p-1)^l \cdot l!<\left(n^{\frac{1}{3}}\right)^{\sqrt[8]{n}}$, for large enough $n$.~\footnote{\includegraphics{image001.png}}

Let $C$ be a monotone circuit computing CLIQUE $(n, \sqrt[4]{n})$.
We apply the approximators at each gate of $C$ iteratively (using structural induction as above).
The output gate of $C$ thus is written as  $\operatorname{CC}\left(X_1, \ldots, X_m\right)$ for some $m \leqslant M$ and $\left|X_i\right| \leqslant l$.
Based on the above claim, we have two cases:

\case 1 $\operatorname{CC}\left(X_1, \ldots, X_m\right) $ is identically 0. 
Thus, the number of \emph{false negatives} $\operatorname{CC}\left(X_1, \ldots, X_m\right)$ has in total is the number of all possible positive instance, namely all possible $k$-cliques, which is $\binom{n}{k}$.
By \Cref{lem:new-false-negatives},  each approximation step introduces at most $M^2\cd\binom{n-l-1}{k-l-1}$ new false negatives. Therefore,  we have:

$$
|C| \geq \frac{\binom{n}{k}}{M^2\cd\binom{n-l-1}{k-l-1}} \geq \frac{1}{M^2}\left(\frac{n-l}{k}\right)^l \geq n^{c \sqrt[8]{n}}, \text{~~for $C=1/12$.}
$$


\case 2 $\operatorname{CC}\left(x_1, \ldots, x_n\right)$ has at least $\frac{1}{2}(k-1)^n$ \emph{false positives} (half of the total number of  $(k-1)$-colourings).
By \Cref{lem:new-false-positives},
each approximation step introduces at most $ M^2 \frac{(k-1)^n}{2^p}$
 new false positives. Therefore,  we have:
$$
|C| \geq \frac{\frac{1}{2}(k-1)^n}{M^2 \frac{(k-1)^n}{2^p}}=\frac{2^p}{2 \cdot M^2}>n^{c\cdot \sqrt[8]{n}}, 
\text{~~for~}c=\frac{1}{3}.
$$
\end{proof}



% Enable watermark from this point onward
\newpage\SetWatermarkText{DRAFT}


% _____________________________________________________________________
\subsection{Proofs of \cref{lem:new-false-positives} and \cref{lem:new-false-negatives}: the approximator steps}
%_______________________________________________________________________
\para{Base Case} $|C|=1$, i.e., $C$ consists of only a single input gate $g_{ij}$. Recall that $g_{i j}$ is an input gate denoting whether $(i, j) \in E$, for $i, j \in V$.
That is, if there is an edge between $i$ and $j$ in the input graph $G$.

This is an easy case: We need to show an approximator $\operatorname{CC}(X_1, \ldots, X_m)$ with $m \leq M$ and $\left|X_i\right| \leqslant l$, for all $i \in[m]$ that approximates $g_{i j}$ (without introducing too many errors; we shall count precisely the number of potential errors later).
But the circuit $C C(\{i, j\})$ is $g_{i j}$ by definition. (Hence, no errors here.)




\para{Induction step }

Given two approximators
$\operatorname{CC}(\mathcal X)$ and 
$\operatorname{CC}(\mathcal Y) 
$, with $\mathcal{X}=\{X_1, \ldots, X_m\}$, 
$\mathcal{Y}=\{Y_1, \ldots, Y_{m^{\prime}}\}$, $m\le M$, and $\left|X_i\right| \leq \ell $, for all $i\in[m]$, $m'\le M $ and $\left|Y_i\right| \leq \ell$, for all $i\in[m']$, we wish to construct the  approximators  for computing $CC(\mathcal{X}) \vee CC(\mathcal{Y})$, and $\operatorname{CC}(\mathcal X) \wedge 
\operatorname{CC}(\mathcal Y)$, respectively.

\case{1}
$\lor$-gate.

\textit{Naive (wrong) attempt}: $\operatorname{CC}(\mathcal X)
\lor \operatorname{CC}(\mathcal Y)$ is approximated by $\operatorname{CC}(\mathcal X \cup \mathcal Y)$. That is, $\operatorname{CC}\left(X_1, \ldots, X_m, Y_1, \ldots, Y_m\right)$. At first glance this is a good solution because it does not introduce any errors (why?). But there is a \textit{problem}: what if $m+m^{\prime}>M$?


\textit{Solution}: We need to cleverly \emph{reduce} the number of sets $X_1, \ldots, X_m, Y_1, \ldots, Y_{m^{\prime}}$. To do this we use a combinatorial lemma called The Sunflower Lemma.

\subsubsection{The Sunflower Lemma}

\begin{figure}
    \centering
        \includegraphics[width=0.75\linewidth]{images/sunflower-lemma.png}
    \caption{From https://theorydish.blog/2021/05/19/entropy-estimation-via-two-chains-streamlining-the-proof-of-the-sunflower-lemma/ by Lunjia Hu}
    \label{fig:enter-label}
\end{figure}



%Let $U$ be some universe, namely a set of elements (e.g., nodes). Let $Z=\{Z_1,\dots,Z_M\}$ be a family of sets from the universe, i.e., $Z_i\subseteq U$, for each $i$, with $M$ some natural number. %We call the family $P$ a \emph{sunflower} if  $\left\{P_1, \ldots, P_p\right\}$ called \emph{petals}, each $\left|P_i\right| \leqslant \ell$ where $\ell=\sqrt[8]{n}$, such that all pairs $P_i \neq P_j$ in the family share the \emph{same} intersection, called the \emph{core} of the sunflower. In other words, there is a set  $ P_i\cap P_j = Core$

\begin{svgraybox}
\begin{definition}[Sunflower] Let $U$ be some universe, namely a set of elements (e.g., nodes). Let $P=\{P_1,\dots,P_p\}$ be a family of distinct sets from the universe, i.e., $P_i\subseteq U$, for each $i$, with $p$ some natural number. 
We call the family $P$ a \emph{sunflower} if each all pairs $P_i \neq P_j$ in the family $P$ share the \emph{same} intersection, called the \emph{core} of the sunflower.
In other words, there is a (possibly empty) set $\mathrm{core}\subseteq U$, such that for all $i\neq j$,  $ P_i\cap P_j = \mathrm{core}$.
If $P$ is a sunflower we call the $P_i$'s the \emph{petals} of the sunflower $P$.
\end{definition}
\end{svgraybox}
Note: It's okay if the core is the empty-set! This means all petals are (pairwise) disjoint.


\begin{svgraybox}
\textbf{Sunflower Lemma} (Erd\"os-Rado): For every $\ell, p$, let $Z$ be a family of more than $M=(p-1)^\ell \cdot \ell!$ non-empty sets each of size $\leqslant \ell$ (over some universe $U$). Then, $Z$ contains a sunflower of size $p$. In other words, $Z$ contains $p$ sets $\left\{P_1, \ldots, P_p\right\}$, each $P_i$ has size $\leqslant \ell$, and the intersection of every pair $P_i \neq P_j$ is fixed: $P_i \cap P_j=P_{i'} \cap P_{j'}$, for all $i \neq j \neq i' \neq j^{\prime}$.
\end{svgraybox}



\begin{proof}[Proof of the Sunflower Lemma]
By induction on $\ell$.

\Base  $\ell=1$. Thus the statement we need to show is that $p$ different singletons form a sunflower. Which is true (the core is $\varnothing$ ).


\induction $\ell>1$. Consider a family $D \subseteq Z$ of pairwise disjoint sets that is maximal in the sense that if we add any new set in $Z$ to the family $D$, the sets in $D$ are not pairwise disjoint anymore. That is, every set in $Z \backslash D$ intersects some set in $D$.

\case 1 If $D$ contains $\geq p$ sets then $D$ is a sunflower with empty core, and we are done.

\case 2 Otherwise, let $E$ be the \emph{union} of all sets in $D$.
Since $|D|<p$, i.e., $D$ contains less than $p$ sets, and each set in $D$ has size $\le \ell$, we know that $|E| \leqslant(p-1)\cd\ell$.

\medskip 

Moreover, $E$ intersects every set in $Z$ by assumption.
Since $Z$ has more than $M$ sets by assumption, and each set intersects some element of $E$, there exists an element $d \in E$ that intersects $>\frac{M}{(p-1) l}=(p-1)^{l-1} \cdot(l-1)!$ sets in $Z$.


\begin{remark}
If a set $E$ intersect all sets in a family of sets $X_1, . ., X_M$ then there is an element in $E$ that appears in $\geq \frac{M}{|E|}$ sets $X_i$.
Otherwise, each element in $E$ appears in $<\frac{M}{|E|}$ sets $X_i$. Thus, $E$ intersects $<|E| \cdot \frac{M}{|E|}=M$ sets $X_i$, which is a contradiction to the assumption.
\end{remark}


Consider

$$
Z^{\prime}:=\{z \backslash\{d\} \mid z \in Z \text { and } d \in z\} .
$$


We know that $Z^{\prime}$ contains more than $M^{\prime}=(p-1)^{l-1} \cdot(l-1)!$ sets.
By \emph{induction hypothesis} (since $M^{\prime}$ is "$M$ with $\ell$ decreased by one"), $Z^{\prime}$ contains a sunflower denoted $\left\{P_1, \ldots, P_p\right\}$ with $\left|p_i\right| \leq \ell-1$, for all $i$.
Hence, $\left\{P_1 \cup\{d\}, \ldots, P_p \cup\{d\}\right\}$ is a sunflower in $Z$.
This concludes the Sunflower Lemma's proof.
\end{proof}


% _____________________________________________________________________
\subsubsection{Induction step of approximator construction continued: approximating OR and AND using Plucking}
% _____________________________________________________________________
- By the Sunflower Lemma, every family of $\geq M$ nonempty sets, each of cardinality $\leqslant l$, then we can find a sunflower in it with $l=\sqrt[3]{n}(p \approx \sqrt[3]{n}) M=(p-1)^l \cdot \ell!$.
- Plucking a sunflower: replacing all petals by their core. 4 sets

Corollary: If we have $>M$ sets in a family, by repeated plucking we can reduce the number of sets to $\leq M$ (if we cant apply plucking. anymore we know by the Sunflower Lemma that the number of sets is $\leqslant M$ ).
pluck (z): the result of repeated plucking of a family of sets $Z$, until $|Z| \leqslant M$.
Definition (Approximate OR and AND): foxily of sets The approximate $-V$ of two crude -circuits $C C(x)$ and $\operatorname{cc}(y)$ is: $\operatorname{cc}($ pluck $(x \cup y))$.

The approximate $-\Lambda$ of $C C(x)$ and $C C(Y)$ is: $\operatorname{cc}\left(\right.$ pluck $\left\{X_i \cup Y_j:\left|X_i \cup Y_j\right| \leqslant l\right.$ and $\left.\left.X_i \in \chi, Y_j \in Y\right\}\right)$





$V$-approximator of $\operatorname{CC}(x) \vee \operatorname{CC}(y)$ introduces
1) a false negative:

A k-clique $G=(v, E)$ s.t.: $(\operatorname{cc}(x) \operatorname{vcc}(y))(G)=1$
2) a false positive:
$\operatorname{cedpluck}(x \cup y)(G)=0$
$V$-aterosimator
$A(k-1)$-coburing $G=(v, E)$ s.t.

$$
\begin{aligned}
& (\operatorname{cc}(x) \vee \operatorname{cc}(y))(G)=0 \\
& \frac{\operatorname{cc}(p l u c k(x \cup y))(G)}{V \text {-approvinator }}=1
\end{aligned}
$$


1 -apporimator of $\operatorname{CC}(x) \wedge \operatorname{CC}(y)$ introduces
1) a false negative:

A k-clique $G=(v, E)$ s.t: : $(\operatorname{cc}(x) \wedge \operatorname{cc}(y))(G)=1$
2) a false positive: $c c\left(P \mid\right.$ luck $\left.\left\{x_i\left|y_i:\left|x_i \cup y_i\right| \leq l, x_i \in X, y_i \in\right\}\right\}\right)=0$

$$
\left.c c\left(p l u c k\left\{x_i \cup Y_j:\left|x_i \cup y_0\right| \leq l, x_i \in \chi, y_i \in\right\}\right\}\right)=1
$$




Lemma 1: Each approximation step introduces at most $M^2 \frac{(k-1)^n}{2^p}$ false positives.

Pe of F: Case 1: OR-approximator
We start with $\overline{C C}\left(x_1, \ldots, x_m\right)$ and $\operatorname{CC}\left(y_1, \ldots, y_{m_m}\right)$ and consider a false positive introduced by

$$
\operatorname{cc}\left(\text { pluck }\left(X_1, \ldots, X_m, Y_1, \ldots, Y_{m^{\prime}}\right)\right) \text {. }
$$


That is, a $G=(v, E)$ s.t.,

$$
\operatorname{cc}\left(x_1, \ldots, x_m\right)(G)=0, \quad \operatorname{cc}\left(y_1, \ldots, y_m\right)(G)=0
$$

and

$$
\operatorname{cc}\left(\operatorname{pluck}\left(X_1, \ldots, X_m, Y_1, \ldots, Y_{m^{\prime}}\right)\right)(G)=1 .
$$


We consider each plucking involved in (2), and bound from above the number of false positive introduced by this plucking. (Note this is the only reason a false positive can be introduced.)


 - Consider a single plucking : replace sunflower $\left\{z_1, \ldots, z_p\right\}$ by its core $Z$.

By (1): $Y_i^{\prime} s \& X_i^{\prime} s(\&, ' s)$
are all hon-cliques
Thus, by (2)
$Z$ is a clique
and every petal $z_i$
has two nodes w/ the $\frac{\text { same colour! }}{L \operatorname{bg}(1)}$



\begin{figure}[H]
    \centering
    \includegraphics[width=.6\linewidth]{images/clique3.png}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}


- We count the number of such colourings.
- We do this probabilistically:

Choosing a $(k-1)$-colouring of the nodes randomly and independently, what's the probability every $z_i$ has repeated colours but $z$ does not?
- As before, let $R(X)$ be the probability that $X$ has repeated colours.


$\qquad$ As before, let $R(X)$ be the probability that $X$ has repeated colours.

$$
\operatorname{pr}\left[R\left(z_1\right) \wedge \ldots \wedge R\left(z_p\right) \wedge \neg R(z)\right] \leqslant \operatorname{pr}\left[R\left(z_1\right) \wedge \ldots \wedge R\left(z_p\right) \mid \neg R(z)\right]$$

$$
\begin{aligned}
& =\prod_{i=1}^p \operatorname{Pr}\left[R\left(z_i\right) \mid \neg R(z)\right] \\
& \leqslant \prod_{i=1}^p \operatorname{Pr}\left[R\left(z_i\right)\right]
\leqslant \frac{1}{2^p}
\end{aligned}
$$

$$
\operatorname{Pr}[A \| B]=\operatorname{Pr}[A \mid B] \cdot \operatorname{Pr}[B]
$$


Li's don't have common nodes, except those in $Z$.

We've seen before that

$$
\operatorname{Pr}[R(x)] \leqslant \frac{1}{2}
$$

Probability of repetition of colours is increased if we doit restrict ourselves to colourings w/ no repetitions in $z \subseteq Z_i$.


Finally, since the approximation step entails up to $\frac{2 M}{p-1}$ pluckings (each plucking decreases the number of sets by $\mathrm{p}-1$, and there are no more than 2 M sets when we start), the lemma holds for the OR approximation step: because e

$$
M^2 \cdot \frac{(k-1)^n}{2^p} \geq \frac{2 M}{p-1} \cdot \frac{(k-1)^n}{2^p}
$$


Cont. of proof of Lemma 1

Consider now an AND approximation step of approximators $C C(x)$ and $C(y)$. It can be broken down in three phases: First, we form $C C(\{X \cup Y: X \in \chi, Y \in Y\})$; this introduces no false positives, because any graph in which $X U Y$ is a clique must have a clique in both and $Y$, and thus it was accepted by both constituent approximators. The second phase omits from the approximator circuit several sets (those of cardinality larger than $\ell$ ), and can therefore introduce no false positives. The third phase entails a sequence of fewer than $\mathrm{M}^2$ plucking, during each of which, by the analysis of the OR case above, at most $2^{-\mathrm{P}}(\mathrm{k}-1)^{\mathrm{n}}$ false positives are introduced. The proof of the lemma is complete: because at total we introduced $\leq M^2 \cdot \frac{(k-1)^n}{2^p}$ false positives



\para{Proof of Lemma XXX}
We need to show that each approximation step introduces $\leqslant M^2\binom{n-l-1}{k-l-1}$ false negatives.

\begin{proof}[Proof of Lemma XXX]
Case $V_{:}$a false negative:
A $k$-clique $G=(v, E)$ st.: $(\operatorname{cc}(x) \vee \operatorname{ccc}(y))(G)=1$ (

$$
\underbrace{\operatorname{colpluck}(x \cup y)(G)}_{V \text {-aproximator }}=0
$$


This is impossible, because plucking only deletes sets and make them smaller; hence if (1) holds then (2) cannot.
Case 1:
A false negative:
A $k$-clique $G=(v, E)$ st: : $(\operatorname{cc}(x) \wedge \operatorname{cc}(y))(G)=1$

$$
\left.c\left(P / \mid u c k\left\{x_i \cup Y_j:\left|x_i \cup Y_i\right| \leq l, x_i \in \chi_{,}, Y_i \in\right\}\right\}\right)=0 \text { (2) }
$$


In the first stop we replace $c a(x) \wedge c(y)$ by $C C(\{X \cup Y: x \in X, y \in Y\})$.
Hence, if $G$ is a $K$-clique and both $X$ and $Y$ are each cliques in $G$, it must be that $X \cup Y$ is also a clique in $G(w h y$ ? ). Thus, no false negatives are introduced in this step.


$\frac{\text { (cont.) }}{\text { We next delete all sets } \overbrace{X_i \cup Y_j}^{\text {denoted }} Z}$ larger than $l$. This introduce several false negatives:
All $k$-cliques $G$ that contain $Z$.
We calculate an upper bound on these false negatives: There are precisely $\binom{n-|z|}{k-|z|}$ k-cliques er that contain $z$ (as part of the clique). Since, $|z|>l$, the upper bound on the false negatives introduced by each deletion is $\binom{n-l-1}{k-l-1}$.

Since there are $\leqslant M^2$ deletions of sets, (because $|\chi|=|y|=M$ ), we get that the number of false negatives introduced by $a_n 1$-approximation step is $\leqslant M^2 \cdot\binom{n-t-1}{k-l-1}$.

\end{proof}

