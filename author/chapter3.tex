%%%%%%%%%%%%%%%%%%%%% chapter2.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%
%\motto{Use the template \emph{chapter.tex} to style the various elements of your chapter content.}










\chapter{Constant Depth Circuit Lower Bounds}
\label{hastad} % Always give a unique label
% use \chaptermark{}
% to alter or adjust the chapter heading in the running head

%\section{Defining constant depth circuits}

Recall \Cref{def:boolean-circuit} of Boolean circuits.
We define the \emph{\textbf{depth}} of a Boolean circuit $C$ as 
the maximal length (i.e., number of edges) in a path from 
a leaf to the output node.

Here is an example of a circuit of depth 3:
\begin{figure}
    \centering
    \includegraphics[width=0.2\linewidth]{images/depth_3_circuit.png}
    \label{xxx}
\end{figure}

\newcommand{\complexityclass}[1]{\sf{#1}}
\newcommand{\ACZ}{\ensuremath{\complexityclass{AC}^0}}


Recall that we are interested in the  study of the asymptotic size of circuit families, not a single circuit. That is, we want to consider how circuit size grows when the number of inputs $n$ grows. In this chapter we shall focus on circuit families $\{C_n\}_{n=1}^\infty$
whose number of inputs bits $n$ grows, their size $|C_n|$ grows polynomially while their depth is \emph{fixed} throughout the family, namely is a constant $c$ independent of $n$. 

%The class of Boolean functions computed by such circuit families is denoted $\ACZ$, and the class of such constant depth cir

%We can consider circuits of constant depth. This means that the depth of the circuit is \emph{independent of the number of inputs} $n$. In other words,  while $n$ the number of input gates can grow to infinity in the Boolean circuit family $\{C_n\}_{n=1}^\infty$, the depth of each circuit $C_n$ stays the same!


If the depth of the circuit is constant and the fan-in of gates is at most two, then the number of functions we can compute with such constant-depth circuits is constant. For example, the number of variables (appearing as leaves, i.e., input nodes) is constant that way, so for a large enough number of inputs $n$, we will not be able to compute functions that read all the $n$ inputs. This means that the model is \emph{not  complete}: for a constant $d$, a depth-$d$ circuit cannot compute all Boolean functions over $n$ inputs for every $n \in \mathbb{N}$.

To solve this problem and make the model of constant-depth
circuits meaningful we allow \textbf{\textit{unbounded} \emph{fan-in}} gates: unbounded fan-in AND gates: $\left(\varphi_1 \wedge \varphi_2 \wedge \ldots \wedge \varphi_\ell\right)$, and unbounded fan-in OR gates
$\left(\varphi_1\lor \cdots \lor \varphi_{\ell}\right)$, where the $\varphi_i$'s are circuits (of constant depth) by themselves, with possibly \emph{joint} nodes. 
\begin{figure}
    \centering
    \includegraphics[width=0.2\linewidth]{images/AND.png}
    \includegraphics[width=0.25\linewidth]{images/OR.png}
    \caption{Illustration of unbounded fan-in AND and OR gates.}
\end{figure}



\newcommand{\depth}{\ensuremath{{\rm depth}}}

\begin{definition}[Depth-$d$ circuit family, constant-depth family]
\label{def:constant-depth-circuit}
A circuit family 
$\left\{\mathrm{C}_n\right\}_{n=1}^\infty$ with unbounded fan-in $\land,\lor$ gates and fan-in one negation gate is said to be of \emph{depth-$d$}, if the depth of $C_{n}$ is $d$, for all $n\in\N$. We denote by $\depth(C_n)$ the depth of the circuit $C_n$.
If $d$ is a constant (independent of $n$) we call $\left\{C_n\right\}_{n=1}^\infty$ a \emph{constant-depth family of circuits}.
\end{definition}

\textbf{Notation}: \ACZ\ denotes the complexity class 
consisting of all the functions $f:\{0,1\}^* \rightarrow\{0,1\}$ computable by  constant-depth circuit families  (equivalently, all decision problem decidable by \ACZ\ circuit families).

\para{Alternating Formulas}


We are going to use the following ``normal-form'' for constant-depth circuits. This is done for the sake of simplicity, and does not change the size of the circuits significantly, as we remark below.


A Boolean circuit is said to be an \emph{alternating  formula} if it is a formula (i.e., the underlying circuit graph is a tree), and every OR gate is followed by an AND gate followed by an OR gate, etc. Moreover, all negations in an alternating formula are pushed to the bottom of the circuit: that is, a negation node can only be connected to an input node (and that input node is not connected to any other node except a negation node). 
Here is an illustration:
\includegraphics[width=0.3\textwidth]{image004.png}


% Always give a unique label
% and use \ref{<label>} for cross-references
% and \cite{<label>} for bibliographic references
% use \sectionmark{}
% to alter or adjust the section heading in the running head

For simplicity, we can assume that a constant-depth circuit has two types of inputs: every variables $x_i$ has a negative copy $\overline x_i$ (thus, getting rid of the need to specify a negation gate explicitly).  

Note that in an alternating formula every \emph{layer} (equivalently, \emph{level}) of the circuit, namely, nodes of a given depth $l$, are all of the same type: either all $\land$, all $\lor$ or all input variables (positive or negative).



We shall assume that all \ACZ\ circuits are alternating formulas:


\begin{tcolorbox}[colframe=white, colback=gray!11, boxrule=0mm, sharp corners]
 \textbf{Important:} From now on, when speaking about constant-depth circuits (i.e., \ACZ\ circuits), we assume by default the fan-in of $\lor, \land$ gates  is \emph{unbounded} ($\neg$ has fan-in one), and the circuits are \emph{alternating formulas}, and the \textbf{depth} is defined as maximal number of alternations between $\land$ and $\lor$ gates (or vice versa), namely the number of layers in the formula,  excluding the bottom layer which contains only the input variables (positive or negative).  
\end{tcolorbox}

\begin{exercise}
Show that every depth-$d$ circuit can be transformed into an alternating formula of depth-$O(d)$ circuit and with all negation gates on the leaves, with a polynomial size increase only.

\noindent---

\noindent{\small Hint: we can "unwind" the circuit into a formula (a tree; that is, if we used a subcircuit more than once [namely, it has out-degree more than 1], we copy this subcircuit for each such usage). Because the depth is constant this will increase the size of the circuit by at most a polynomial (exponential in a constant---i.e., polynomial). To make the formula alternating, first push all the negation to the bottom, using \emph{de Morgan rules}. And then, add dummy edges and gates to make every path from leaf to the output gate alternating between OR and AND. These all incur a polynomial increase in size.
}
\end{exercise}



\newcommand{\Parity}{\ensuremath{\operatorname{PARITY}}}

%\section{Lower Bounds Against Constant-Depth Circuits for PARITY}

\section{The PARITY Function}

The function PARITY determines the parity of the total number of ones in the input bits:

\begin{tcolorbox}[colframe=white, colback=gray!11, boxrule=0mm, sharp corners]
$
\Parity(x_1,\dots,x_n): =
\begin{cases} 
1 & \text{if the number of 1's in } (x_1, x_2, \dots, x_n) \text{ is odd}, \\
0 & \text{if the number of 1's in } (x_1, x_2, \dots, x_n) \text{ is even}.
\end{cases}
$
\end{tcolorbox}

Equivalently, $\Parity$ is defined as the XOR of the input bits, denoted $x_1\parity\cdots\parity x_n$. Another equivalent definition is using modulus: $x_1+\cdots+x_n = 1 \mod 2$.

We denote by $\Parity_n$ the \Parity\ function restricted to inputs of length precisely $n$.

The main result in this chapter is the following:


\begin{tcolorbox}[colframe=white, colback=blue!11, boxrule=0mm, sharp corners]
\begin{theorem}[\ACZ\ lower bound for \Parity]
\label{thm:ACZ-lower bound}
Every circuit family of depth-$d$ computing the $\Parity_n$ function requires size $2^{\Omega\left(n^{\nicefrac{1}{(d-1)}}\right)}$.
\end{theorem}
\end{tcolorbox}



Note: this means that if $d$ is constant, then the size lower bound is exponential: $2^{\Omega\left(n^{\nicefrac{1}{(d-1)}}\right)}=
2^{\Omega\left(n^k\right)}$, for some constant $0<k<1$.

The theorem is a consequence of the fact that every function in \ACZ\ can be reduced to a constant function by setting relatively few variables to constants.
The proof uses the Random Restriction Method described in the sequel.

\section{The Skeleton of the Lower Bound Argument}



\begin{tcolorbox}[colframe=white, colback=gray!5, boxrule=0mm, sharp corners]
\textbf{Upshot of argument}: The idea is to randomly restrict (assign) the variables of the circuit (of which we want to show it cannot compute \Parity). If the circuit is small, such a restriction collapses (gradually, layer-by-layer) the depth of the circuit to a depth-2 circuit. If the depth of the initial circuit was constant, this gradual step-by-step depth reduction needs to be done only for a constant many times. If we made random assignments (restrictions) only for a constant many times, we can rest assure (assuming each step did not assign too many variables) that with high probability we have not assigned (i.e., fixed) too many variables. Hence, with high probability the function $\Parity$ under these restrictions still has a lot of free variables. But such $\Parity$ with sufficiently many free variables cannot have small depth 2 circuits (this we show by an auxiliary elementary argument).
\end{tcolorbox}

\includegraphics[width=0.9\textwidth]{images/image007.png}


\begin{note} Notice the similarity with the monotone circuit lower bound proof: we use structural induction on the circuit purported to correctly compute \Parity. \end{note}

\medskip
Here are the main steps of the argument with slightly more detail.

\para{Random Restriction Method: Structure of Argument}

\begin{enumerate}
\item By  way of contradiction, start with a purported small depth-$d$ circuit $C$ for $\Parity_n$.
We are going to assign to some of the input variables of the circuit $C$ (and hence, implicitly also to the function $\Parity_n$ that the circuit  allegedly  computes), random 0,1 values. We show the result of these assignments leads to conflicting effects: with high probability  the circuit restricted to these randomly chosen partial assignments becomes a circuit that computes a very simple function; but on the other hand,  the function that survives after all these restrictions is still the $\Parity$ function (only of a somewhat smaller number of variables) and so it is easy to show that such a simple circuit cannot compute this function.

\item Let $\overline x$ be $n$ variables $x_1,\dots,x_n$. Pick a random assignment $\rho_0: \overline x \rightarrow\{0,1, *\}^n$ with ``$x_i \rightarrow *$'' meaning that $x_i$ is \emph{un-assigned}, namely, stays a \emph{free} variable. We make sure that enough $x_i$'s stay  free, as this is crucial to get our contradiction.
\label{it:two-165}

\item Depth Reduction Step: 
%\item   $f$ is resilient in the sense that $f\rst\rho
%%\not\equiv 0$;

\begin{enumerate}
\item 
If $C$ is a small circuit then the \emph{function} computed by $C\rst\rho_0$, namely the restriction of $C$ to the assignment $\rho_0$, has a depth  $d-1$ circuit of small size. 

\item
Sequentially we assign new further random restrictions $\rho_1,\dots,\rho_{d-3}$
 (extending $\rho_0$) so that we reduce the depth of $C$ by 1 each time, until the depth is 2.
\end{enumerate}

\item 
At this point, we end up with a circuit $C^{\prime}$ of depth 2, namely a $k$-DNF or a $k$-CNF, for some small $k$ that computes the function 
$\Parity_n\rst\rho_0\rho_2\ldots\rho_{d-3}$.

\item
We now use an auxiliary statement to reach a contradiction: since we made sure that each assignment $\rho_i$ leaves enough variables free, 
$\Parity_n\rst_{\rho_0\rho_2\ldots\rho_{d-3}}$ is the PARITY function on $k'$ variables, i.e., $\Parity_{k'}$. But this is still a relatively "complex" function:
in particular, it is a very simple argument to show that there are no $k$-CNF formulas for $\Parity_{k'}$, whenever $k'>k$, which will conclude the argument.
\label{it:final-step:269}
\end{enumerate}


\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{images/hastad-sitter-600.jpg}
    \caption{Johan H\aa stad}
\end{figure}


\subsection{Depth Reduction Step: the Switching Lemma}

We shall use the following notation.
\begin{itemize}
    \item \textit{Boolean (propositional) variables:} \(x_1, x_2, \ldots \).
    \item \textit{Literals:} \(x_i\) or \(\neg x_i\) (i.e., a variables or its negation).\\
    (We sometimes write a negation of a variable  \(x_i\) as $\overline {x_i}$.)
    \item \textit{Clause:} A disjunction of literals.\\
    Example: \((x_1 \lor \neg x_2 \lor x_3)\)
    \item \textit{CNF:} Conjunctive Normal Form formula, namely a conjunction  of clauses.\\
    Example: \((x_1 \lor \neg x_2 \lor x_3) \land (x_1 \lor x_4 \lor x_5) \land (x_2 \lor \neg x_7 \lor \neg x_{20} \lor \neg x_{102})\)
    \item \textit{DNF:} Disjunction Normal Form formulas, namely a disjunction of ANDs of literals (an AND of literals is sometimes called \emph{a term}.\\
    Example: \((x_1 \land x_4 \land x_7) \lor (x_8 \land \neg x_9 \land x_{10}) \lor (x_3 \lor \neg x_2)\)
    \item \textit{$t$-clause:} A clause with at most \(t\) literals.\\
    Example: 3-clause: \((x_1 \lor \neg x_2 \lor x_7)\)
    \item \textit{$t$-CNF:} A CNF with all clauses being \(t\)-clauses.
    \item \textit{$s$-DNF:} A DNF with all terms  having at most \(s\) literals.
\end{itemize}


The below figure illustrates a single Depth Reduction Step. Such a step is also called  ``switching'', as it switches a $t$-CNF into an $s$-DNF, or vice versa.

\includegraphics[width=0.7\textwidth]{images/image008.png}

\begin{note} We can always turn a $t$-CNF into an $s$-DNF for some \textit{big} $s$ (by distributing out \(\lor, \land\) using de Morgan's rules).
But then we've not maintained the invariant that the bottom level has fan-in \(\leq s\) (namely, \(s\) is small enough).
In other words, we will end up with a depth-2 circuit, but of huge size computing \(f\rst_\rho\). This will not give us a contradiction since every function is computable by an exponential-size depth-2 circuit.
\end{note}

\para{Examples of Assignments and their Consequences}
\emph{How can we  turn a $t$-CNF into an $s$-DNF while keeping $s$ small}?
We can turn the $t$-CNF to a (general) DNF using de Morgan rules (just distribute $\wedge$ over $\vee$), for example
\[
(x_1 \vee \bar{x}_2) \wedge (x_3 \vee \bar{x}_4) \equiv (x_1 \wedge x_3) \vee (x_1 \wedge \bar{x}_4) \vee (\bar{x}_2 \wedge x_3) \vee (\bar{x}_2 \wedge \bar{x}_4),
\]

and then try to \emph{eliminate literals} in big ANDs by using assignments to the literals in those terms. This may reduce indeed the size of terms, and if we do this enough we may be able to reduce all the terms that are wider than $s$.
 
 
Consider the following DNF:
\[
(x_1 \wedge x_2 \wedge \bar{x}_3 \wedge \bar{x}_5 \wedge \bar{x}_8) \vee ( \dots \wedge \bar{x}_9 \wedge \dots)\lor\dots.
\] We can assign $1$ to $x_2$ and $1$ to $x_9$. In the first conjunct (i.e., the first AND or term) it results in the \emph{deletion of $x_2$}, while in the second conjunct, it results in \emph{deletion of the whole conjunct}:

Assigning $1$ to $x_2$:
\[
(x_1 \wedge x_3 \wedge \bar{x}_5 \wedge \bar{x}_8) \vee ( \dots \wedge \bar{x}_9 \wedge \dots )
\]
Assigning $1$ to $x_9$:
\[
(x_1 \wedge x_3 \wedge \bar{x}_5 \wedge \bar{x}_8) \vee \underbrace{( \dots\wedge 0 \wedge \dots)}_{\text{false}} 
\]





\begin{note}\
\begin{enumerate}
    \item Literal $l_j \from 0$ (i.e., assigning $0$ to literal $l_j$)
    \[
        (l_j \wedge D) \equiv \text{false}.
    \]
    So the whole AND disappears.
    
    \item Literal $l_j \from 1$:
    \[
        (l_j \wedge D) \equiv D.
    \]
    So we reduced the size of the AND by $1$.
\end{enumerate}
\end{note}

But how can we choose the assignment of $0$ and $1$'s to variables that guarantees eliminating all the large conjuncts, without fixing all the variables in the DNF formula (recall that by \Cref{it:two-165} above we need to keep sufficiently many variables $x_i$ free)?
The idea is to use the \emph{probabilistic method}: choosing a \emph{random} $g: \overline x \to \{0,1, *\}^n$ will lead with \textbf{positive} probability to an assignment that eliminates \emph{all} large conjuncts, while leaving enough variables free. When an event occurs with a positive probability, in this case the event of choosing an assignments with the desired properties, it means that there \textbf{exists} at least one such assignment. 
%This will be enough for our purpose: the existence of such an assignment is sufficient to conclude our proof, because our argument hinges on a proof by contradiction: if there exists a small constant-depth circuit for PARITY, then there \emph{exists} also a desired assignment (formally, a sequence of assignments) that will yield our contradiction as in \Cref{it:final-step:269}.

\para{Switching via Random Restrictions}

\textbf{Notation:} 
\begin{enumerate}
\item 
 A \textit{restriction} $\rho$ for $l$ variables in $\bar{x}$ is a function $\rho: \bar{x} \to \{0,1, *\}$, assigning either 0, 1 values to the variables or leaving them free, namely $\rho(x_i) = *$ means that $x_i$ is \textit{unassigned} (or ``free'') according to $\rho$.

 
\item For a function $f: \{0,1\}^n \to \{0,1\}$ and a   restriction $\rho: \bar{x} \to \{0,1, *\}$, we write $f\upharpoonright \rho$ to denote the function $f$ where the variables are set according to $\rho$.
%and $\rho(x_i)$ means $x_i$ is \textit{left unassigned}.

If we then apply another restriction $\tau$ to the variables in $f\upharpoonright \rho$, we 
write $f\upharpoonright_{\rho\tau}$ to denote $(f\upharpoonright \rho)\upharpoonright \tau$. In other words,
\[
f\upharpoonright_{\rho\tau} = f\upharpoonright_{\rho \cup \tau}
\]
where $\rho \cup \tau$ is the union of two functions.
\end{enumerate}



\begin{tcolorbox}[colframe=white, colback=green!6, boxrule=0mm, sharp corners]
\begin{definition}[Random Restriction]\label{def:random-rest}
 Let $0< p<1$.
Let $\cR$ be a distribution on partial restrictions $\rho$ to the variables $\overline{x}$ with $|\overline{x}|=n$ , as follows.
$$
\begin{aligned}
& ~~~~~~~~~~~ \quad \operatorname{Pr}\left[\rho\left(x_i\right)=*\right]=p \\
& \operatorname{Pr}\left[\rho\left(x_i\right)=0\right]=\operatorname{Pr}\left[\rho\left(x_i\right)=1\right]=\frac{1-p}{2}.
\end{aligned}
$$
\end{definition}
\end{tcolorbox}
In other words, we pick randomly and independently for each variable $x_i$ with $i\in[n]$, either to leave it free with probability $p$, or to fix it to 1 or 0 with equal probability $(1-p)/2$.


\begin{tcolorbox}[colframe=white, colback=blue!5, boxrule=0mm, sharp corners]
\begin{theorem}[Switching Lemma]\label{thm:Switching-Lemma}
For every positive integers $r,s$ and $n$, if a Boolean function $f$ with $n$ variables is computable by an $r$-DNF and $\rho$ is a random restriction as above with $0<p \leq \frac{1}{9}$, then with probability at most $(9pr)^s$, $f\!\rst_\rho$ \emph{cannot} be computable by  an $s$-CNF.
\end{theorem}
\end{tcolorbox}

The Switching Lemma thus shows that the ``bad'' event of a function computed by an $r$-DNF formula not being able to switch, under  a random restriction,  to a function that can be computed by an $s$-CNF, is small. This would mean that with high enough probability the ``good'' event happens: a random assignment switches an $r$-DNF to an $s$-CNF.
In practice, we usually need to have the probability of the bad event to be \emph{really} small, in this case  exponentially small, so that the probability that \emph{none} of a collection of bad events happens in the random experiment is small (using the union bound), and specifically smaller than 1, to be able to claim there exists an object with the required properties (in our case, an assignment that would switch exponentially many $r$-DNFs).  

However, we note that this probabilistic argument via the union bound  is  \emph{not} sufficient to conclude the argument. We shall also need to make sure that such a ``good'' random assignment  \emph{leaves many variables free}. For this purpose, we will use a common tool in probabilistic analysis and concrete complexity: \emph{concentration bounds} (in our case it will be the Chernoff Bound). We  explain this point in the the next section when proving the lower bound.


\begin{exercise} Show that by applying the Switching Lemma to the function $\neg f$ we can get the same result with the terms ``DNF'' and ``CNF'' interchanged. Namely, the probability that an $r$-CNF does not switches to an $s$-DNF is at most $(9pr)^s$.
\end{exercise}

\section{Proof that PARITY does not have small circuits, using the Switching Lemma}

Recall that we want to prove \Cref{thm:ACZ-lower bound}  stating that every circuit family of depth-$d$ computing the PARITY on $n$ variables requires size $2^{\Omega\left(n^{\nicefrac{1}{d-1}}\right)}$. In this section we are going to assume the Switching Lemma to prove this result. In the next section we shall prove the Switching Lemma itself. 

So, suppose  that $C$ is a depth-$d$ alternating formula   of size $S$ computing $\Parity_n$. We can assume that 
$S< 2^{ n^{\nicefrac{1}{d-1}}}$, as otherwise we finish the proof.


Recall that we assume that $C$ is an alternating formula, with $d$ layers, where in each layer all gates are either $\land$ or $\lor$.  The proof proceeds by first a ``preprocessing step'' in which we use the Switching Lemma to decrease the fan-in of the \emph{bottom} layer gates, followed by   $d-2$ depth reduction steps using the Switching Lemma in which we gradually reduce the depth of the circuit, from bottom to top, by one depth, until we reach a depth-$2$ circuit.
After this, we can conclude the theorem: if the circuit was too small, we end up with a $k$-CNF (or $k$-DNF) computing \Parity\ over more than $k$ variables which is easily shown to be impossible.




% Enable watermark from this point onward
%\SetWatermarkText{DRAFT}



\para{Decreasing Bottom Layer Fan-in}

This step is needed in order to optimise the lower bound, namely to reach the tight lower bound of $2^{\Omega(n^{1/d-1})}$ (see \Cref{note:bottom-explanation}).

Consider the bottom two layers of the given constant-depth circuit. We assumed that the layers alternate between $\land$ and $\lor$, thus the depth-2 sub-formulas  at these bottom two layers are all either DNFs or CNFs. Let us assume  without loss of generality  that they are all CNFs. The bottom fan-in of these CNFs may be large, namely, it may be $\ell$-CNFs for a large $\ell$. 
Since  $\ell$ may be  very large, using the Switching Lemma to turn this $\ell$-CNF to an $s$-DNF will be too expensive, in the sense that applying the Switching Lemma with this $\ell$ would give us probability of $\leq (9p\ell)^s$, but with \emph{large} $\ell$, which later would mean we get a weaker lower bound (e.g., we would get a lower bound of $2^{\Omega(n^{\frac{1}{d}})}$, which is weaker, namely smaller, than our lower bound of $2^{\Omega(n^{\frac{1}{d-1}})}$).
See \Cref{note:bottom-explanation} for an explanation of this. 



For this reason, we shall reduce the fan-in of the bottom layer (i.e., layer 1) directly using the Switching Lemma (while maintaining a lower probability of failure).

\begin{claim}\label{cla:bottom-switch}
An $\ell$-CNF does not turn into a $k$-CNF under a random restriction $\rho$ with probability $\leq (9p_0)^k$, where $p_0$ is the probability a variable is free in the random restriction $\rho$. Specifically, when $k = 2 \cdot \log S$, if we choose $p_0 = \frac{1}{18}$, the probability of failure is at most $ \frac{1}{S^2}$.
\end{claim}

\begin{proof}[Claim]
The below figure illustrates  the simple argument:

\begin{center}
\includegraphics[width=\textwidth]{images/bottom-layer-switch.png}
\end{center}


In particular, we switch the lower 1-DNFs (\emph{in red}) into $k$-CNF with a random restriction, so by the Switching Lemma, the probability the switching fails is 
\[
\leq (9 \cd p_0 \cd1)^k = (9 \cdot \frac{1}{18})^{2 \cdot \log S} = \frac{1}{2^{2 \cd\log S}} = \frac{1}{S^2}.
\]
\
\end{proof}

By this claim, and using the union bound, the probability that at least one $\ell$-CNF in layer 2 does not switch is at most $ S\cd 1/S^2 = 1/S$.



\para{Repeated Depth Reduction Step}
We now can assume we start from a circuit $C{\rst}_{\rho_0}$ with $k$-CNF at the bottom, for $k = 2 \log S$, where $\rho_0$ is the restriction that decreased the bottom layer fan-in above. (For simplicity we use the notation $C$ below, instead of $C{\rst}_{\rho_0}$.)
We go by induction on depth, to prove the following.


\begin{tcolorbox}[colframe=white, colback=red!11, boxrule=0mm, sharp corners]
\begin{lemma}[Induction Statement]\label{lem:induction-switch}
Let $C$ be a depth at least $d$ circuit of size 
$S<
2^{
\sqrt[d-1]{n}
}$ 
with layer-1 (bottom) gates all of fan-in  $ \leq k=2 \log S$ (either  $\land$ or $\lor$),
with $n$ input variables  
$\overline x$ (with positive and negative copies). Assume that the gates in layer $d$ are all $\land$ gates (resp., $\lor$ gates). 
Then, there exists a restriction $\rho: \overline {x} \rightarrow\{0,1, *\} $ with at least  $\frac{p^{d-2} \cdot n}{2^{d-2}} $ free variables, where $ p=\frac{1}{36 \cdot \log S}$, such that  
the function computed by $g{\rst}_{\rho}$ for every $\land$ (resp. $\lor$) gate $g$ in layer $d$ is computable by a $k$-DNF (resp., a $k$-CNF) formula. 
\end{lemma} 
\end{tcolorbox}

\begin{proof}[\Cref{lem:induction-switch}]
We prove it for the case $\land$ in layer $d$. The $\lor$ case is similar.  

\Base $d = 2$. Since the bottom layer is assumed to be of fan-in $\leq k$,  
this case holds immediately.  

\Induction   
We assume the induction statement holds for  $d-1$, and prove it for $d$. The following figure illustrates the argument. 




\begin{figure}[H]
    \begin{center}
    \includegraphics[width=\textwidth]{images/switchinginduc.png}
    \end{center}
    \caption{The setup of the induction step.}\label{fig:depth-reduction-use}
\end{figure}

\begin{enumerate}
    \item 
        In (\textbf{1}) in    Figure~\ref{fig:depth-reduction-use},  we have a depiction of part of the circuit $C$, showing a \emph{single} $\land$ gate in layer $d$ (there  may be other $\land$ gates in this layer, and there may be other gates above it in layers higher than $d$). 


    \item In (\textbf{2}) we see the $\land$ gate in layer $d$ under the restriction $\rho'$, i.e., $C{\rst}_{\rho'}$, with $\rho'$ having $\geq \frac{p^{d-3} \cdot n}{2^{d-3}}$ free variables. Hence, by induction hypothesis its children, which are $\lor$ gates, have turned under $\rho'$ to  $k$-CNFs.

    \item (\textbf{3}) simply merged the $\land$ gate in layer $d$ to its children in layer $d-1$, all of which turned to $\land$ gates themselves. 

    \item Our goal is to get to circuit (\textbf{4}), by applying the Switching Lemma with a random restriction $\rho''$ and then compute an upper bound on the probability that such a switch to $k$-DNF occurs for \emph{all} the gates in layer $d$. We explain this now.
\end{enumerate} 

\smallskip 

% \begin{theorem}[Switching Lemma]
% For every positive integer $r$ and $k$ and $n$,  
% if a Boolean function $f$ with $n$ variables is computable by an $r$-DNF  
% and $\rho$ is a random restriction as above with $0 < p \leq \frac{1}{9}$,  
% then with probability at most $(9pr)^k$, $f_{|\rho}$ cannot be computable  
% by an $s$-CNF.
%\end{theorem}


Using the Switching Lemma with the parameters \( r = s = k = 2 \log S \), with probability \( \leq (9pk)^k \), a \emph{single} $k$-CNF in layer \( d \) does not switch to a $k$-DNF.
By the union bound, with probability \( \leq S\cd(9pk)^k \) the bad event that \emph{at least one} $k$-CNF in layer \( d \) does \emph{not} switch to a $k$-DNF occurs.
We now use another bound to estimate the probability of the event that a random restriction has too few free variables. The tool we use is the important \emph{Chernoff bound}.


\begin{tcolorbox}[colframe=white, colback=gray!11, boxrule=0mm, sharp corners]
\textbf{Chernoff Concentration Bound}.
 
\paragraph{Random variables and expectations.}
Let $\Omega$ be a finite sample space of \emph{atomic (elementary) outcomes}. A \emph{random variable} is a function
\[
X:\Omega \to \mathbb{R}.
\]
So each atomic outcome $\omega\in\Omega$ is mapped to a real number $X(\omega)$.

A basic and important example is a $\{0,1\}$-valued random variable (an \emph{indicator variable}). Given an event $E \subseteq \Omega$, define a random variable $X$ by
\[
X(\omega)=
\begin{cases}
1 & \text{if } \omega\in E,\\
0 & \text{otherwise.}
\end{cases}
\]
Thus $X$ records whether the event $E$ happened.

If a random variable $X$ takes values in a finite set $A\subseteq\mathbb{R}$, its \emph{expectation} is
\[
\mathbb{E}[X] \;=\; \sum_{a\in A}\Pr[X=a]\cdot a.
\]

\paragraph{Linearity of expectation.}
For any random variables $X_1,\dots,X_n$ (not necessarily independent),
\[
\mathbb{E}\Big[\sum_{i=1}^n X_i\Big] \;=\; \sum_{i=1}^n \mathbb{E}[X_i].
\]

\paragraph{Concentration around the expectation.}
Often we consider a sum
\[
X \;=\; \sum_{i=1}^n X_i,
\]
where $X_1,\dots,X_n$ are \emph{independent} random variables taking values in $\{0,1\}$. In that case $\mu=\mathbb{E}[X]$ is the expected number of indices $i$ for which $X_i=1$. The Chernoff bound says that $X$ is very unlikely to differ from $\mu$ by a noticeable multiplicative factor.

\begin{theorem}[Chernoff bound, two-sided form]
Let $X_1,\dots,X_n$ be independent $\{0,1\}$-valued random variables with $\Pr[X_i=1]=p$ and $\Pr[X_i=0]=1-p$. Let
\[
X=\sum_{i=1}^n X_i
\qquad\text{and}\qquad
\mu=\mathbb{E}[X]=pn.
\]
Then for any $0<\delta<1$,
\[
\Pr\big[\,|X-\mu| \ge \delta\mu\,\big]
\;\le\;
2e^{-\delta^2\mu/3}.
\]
\end{theorem}
\end{tcolorbox}



\
\

We apply Chernoff bound to our case as follows.
Every single variable \( x_i \) has a probability \( p \) to stay free under our new restriction  \( \rho'' \).
Hence, the expected number $\mu$ of free variables is \( pN \), where $N$ is the number of (free) variables in $C\rst_{\rho'}$ which is $\ge \frac{p^{d-3} \cdot n}{2^{d-3}}$ by induction hypothesis. Hence, the expected number of free variables is $\mu\ge pN= \frac{p^{d-2} \cdot n}{2^{d-3}}$. 
(Since the event that a variable stays free under \( \rho'' \) is independent from the events that other variables stay free, the expected value is achieved with high probability by the Chernoff bound.)
 In other words, the bad event that the number of free variables are less than half of the expected value $\mu$ is at most $
2e^{-\delta^2 \cdot \frac{\mu}{3}}$, with $\delta = \frac{1}{2}$ and  $\mu = \frac{p^{d-2} \cdot n}{2^{d-3}}.$

The following is thus the upper bound on the probability that at least one bad event happened (using the union bound): 
\begin{equation}\label{eq:487:28}
 S(9pk)^k + 2e^{-\delta^2 \cdot \frac{\mu}{3}}, \quad \text{with} \quad \delta = \frac{1}{2} \quad \text{and} \quad \mu = \frac{p^{d-2} \cdot n}{2^{d-3}},
\end{equation}
where the left most term is by a union bound on all gates, and the rightmost term is the bad event estimated by the  Chernoff bound.\footnote{Note that we have used the union bound on the following bad events, given a random pick of a single assignment $\rho$: $S$ many potential bad events that a gate in layer $d$ does not switch, together with a single bad event that the number of free variables in $\rho$ falls below half the expected number of free variables.
}


%All that is left is to make some computations to get a lower bound on the parameter $S$. 

\Cref{eq:487:28} is equivalent to 
\begin{equation}\label{eq:498:27}
S \left( 9 \cdot \frac{1}{36 \cdot \log s} \cdot 2 \log s \right)^{2 \log s} + 
\underbrace{2 e^{-\frac{\left(\frac{1}{36 \cdot \log S }\right)^{d-2} \cdot n}{2^{d-2} \cdot 3}}}_B.
\end{equation}Let us denote the right summand above by $B$, namely $B=2\cd
e^{-\left(\frac{n}{{(36 \cdot \log S)^{d-2} \cdot 2^{d-2} \cdot 3}}\right)}$.  
Thus, \Cref{eq:498:27} equals
$$
S \cdot\left(\frac{18}{36}\right)^{2 \cdot \log S}+B =S \cdot \frac{1}{S^2}+B =
\frac{1}{S}+
B.
$$
To prove the induction statement it  suffices to show that $1/S+B<1$, hence the probability that none of the bad events occurred is positive. It is enough to prove  that $B<1/2$, since we can assume that the size $S$\ of the circuit is at least $2$ (e.g., because the number of variables $n=2$), meaning that $1/S+B<1$. Thus, we are left to show the following:
\[
e^{\frac{n}{(36 \log S)^{d-3} \cdot 2^{d-1}\cd 3}} > 4
\]
for which it is enough to show that 
$
\frac{n}{(36 \log S)^{d-3} \cdot 2^{d-1}\cd 3}>2,
$
namely
\[
\frac{n}{6 \cdot 2^{d-1}\cd 36^{d-3}} > \log^{d-3} S.
\]
%\[
%n > (36 \log s)^{d-3} \cdot 2^{d-1} \cdot 6.
%\]
Taking the $(d-3)$th root, this is equivalent to showing 
\[
\frac{1}{36}\cd \sqrt[d-3]{\frac{n}{6 \cdot 2^{d-1}}} > \log S,
\]
which means showing
\begin{equation}\label{eq:534:30}
2^{
\frac{1}{36}\cd \sqrt[d-3]{\frac{n}{6 \cdot 2^{d-1}}}
} >  S.
\end{equation}
Since $2^{\frac{d-1}{d-3}} < 2^2 = 4$ (for $d>3$), and by assumption that $ 2^{
\sqrt[d-1]{n}
}>S$,    
$$
2^{
\frac{1}{36}\cd \sqrt[d-3]{\frac{n}{6 \cdot 2^{d-1}}}}>
2^{
\frac{1}{36\cd 6\cd 4}\cd\sqrt[d-3]{n}
}
> 2^{
\sqrt[d-1]{n}
}>S,
$$
and therefore  \Cref{eq:534:30} holds. 
\mbox{}
\end{proof} % Induction step of Switching


\para{Concluding the proof}
Using the Induction Statement \Cref{lem:induction-switch} together with the Bottom Switching Step,
we now conclude the proof of the lower bound.

Given a circuit $C$ of depth $d$ with $n$ variables,
we first apply the Bottom Layer Switching to get a circuit  $C\!\rst_{\rho_1}$ of depth $d$ with layer 2 consisting of $k$-CNF gates. Using Chernoff's bound, with sufficiently high probability  the number of free variables after this step
are at least $(1/36)\cd n$ (we skip this computation).

Then, we have a circuit $C\!\rst_{\rho_1}$ of depth $d$ with layer 2 consisting of  $k$-CNF gates, and at least $n/36$ free variables. We   apply the Induction Statement on this circuit. We reach a circuit $C\!\rst_{\rho_1\rho}$ of depth-2, namely a $k$-CNF (or $k$-DNF) with
$\rho: \bar{x} \rightarrow\{0,1, *\}$ being a restriction
with $\geq \frac{p^{d-2} \cdot (n/36)}{2^{d-2}}$ free variables and $p=\frac{1}{36 \cdot \log S}$, such that the function computed by $g\!\rst_{\rho_1\rho}$ for every $\land$ gate $g$ in layer $d$ (resp., $\lor$ gate) can be computed by a $k$-DNF, with $k=2 \log S$ (resp. a $k$-CNF).
And specifically, when $d$ is the depth of $C$, we know that $C\!\rst_{\rho_1\rho}$ as a whole is computed by a $k$-CNF (in this case, the $d$th layer in $C$ has a single gate). 


But now we get a contradiction whenever \( k \) is less than the number of free variables, which is 
\[
\frac{p^{d-2} \cdot (n/36)}{2^{d-2}}.
\]
This is because every \( k \)-CNF can be made the  constant function 0 with \( k \) assignments of 0/1 values to its variables: Pick one \( k \)-clause, and assign all its literals to zero. For example, if the $k$-clause is 
\[
x_1 \lor \overline{x_2} \lor x_3 \lor \overline{x_4},
\]
 the following assignment falsifies the clause:  
\[
x_1 = 0, \ x_2 = 1, \ x_3 = 0, \ x_4 = 1.
\]
However, Parity for \( r > k \) many inputs \emph{cannot} be made a constant function without assigning all its \( r \) variables. Hence, when the number of free variables
\begin{equation}
\label{eq:660:327}
\frac{p^{d-2} \cdot (n/36)}{2^{d-2}} > k = 2 \log S,
\end{equation}
we get  a contradiction.
In other words, we have:
\[
2 \log S \geq \frac{p^{d-2} \cdot n}{36\cd2^{d-2}}.
\]
We now compute to find out what is the lower bound for $S$:
\begin{align*}
    2 \log S &\ge  \frac{p^{d-2} \cdot n}{36\cd2^{d-2}} 
 = \frac{\left(\frac{1}{36 \log S}\right)^{d-2} \cdot n}{36\cd2^{d-2}} 
    & = \left(\frac{1}{\log S}\right)^{d-2} \cdot n \cdot \left(\frac{1}{36\cd72}\right)^{d-2}.
\end{align*}
Hence, 
\begin{align*}
    \left(\log S\right)^{d-1} &\ge \frac{1}{2} \left(\frac{1}{36\cd72}\right)^{d-2} \cdot n \implies \\
    \log S &\ge \frac{1}{2} \left(\frac{1}{36\cd72}\right)^{\frac{d-2}{d-1}} \cdot n^{\frac{1}{d-1}}
= \Omega \left(n^{\frac{1}{d-1}}\right),
\end{align*}
because 
$(\frac{1}{36\cd 72})^{\frac{d-2}{d-1}}
>
(
\frac{1}{36\cd 72}
)^{\frac{2}{3}}$ (for $d>3$), and hence 
\begin{align*}
     S &=  2^{\Omega ({\sqrt[d-1]{n}})},
\end{align*}
concluding the proof. 
 \hfill $\Box$

% 
% \textbf{And specifically}, when $d$ is the depth of $C$, we know that $C_{\beta}$ as a whole is computed by a k-CNF.
% 
% But now we get a contradiction if $k$ is less than the number of free variables, which is:
% 
% $$ k \geq \frac{p^2 - n}{2} $$
% 
% This is because every k-CNF can be made a constant function (1) with $k$ assignments of 0/1 values to its variables:
% pick one clause, i.e., $x_1 \vee \bar{x_2} \vee x_3 \vee \bar{x_4}$,
% assign the literals in the clause all zero: $x_1 = 0, x_2 = 1, x_3 = 0, x_4 = 1$.
% 
% However, Parity for $k \geq k$ cannot be made a constant function without assigning all its variables.
% Hence, when the number of free variables:
% 
% $$ \frac{p^2 - n}{2} \geq k = 2 \log s $$
% 
% we get a contradiction.
% 
% In other words, we have:
% 
% $$ \log s \geq \frac{p^2 - n}{2} $$
% 
% We now compute to find out what is the lower bound for $s$:
% 
% $$ 2 \log s > \frac{p^{d-2} n}{2^{d-2}} \Rightarrow s > \frac{2^{p^{d-2} n}}{2^{2^{d-2}}} = 2 $$
% 
% $$ \Rightarrow \frac{1}{36 \log s}^{d-2} \cdot n $$
% 
% $$ \Rightarrow 2 \log s > \frac{1}{36^{d-2} (\log s)^{d-2}} \cdot n \cdot c $$
% 
% $$ \Rightarrow (\log s)^{d-1} > n \cdot c $$
% 
% $$ \Rightarrow \log s > \sqrt[d-1]{n \cdot c} $$
% 
% $$ \Rightarrow s > 2^{(n^{\frac{1}{d-1}})} $$
 
% \textcolor{red}{
% \begin{Huge}
% To Be Completed!\end{Huge}}


\begin{note}\label{note:bottom-explanation}
Notice that by  using the Bottom Switching Step with a random restriction with $p_0=1/18$ we got that there exists a good random restriction $\rho_1$ with at least $1/36$ fraction of free variables.  In other words, we  reduce the number of free variable only by a constant factor in this step. This allowed us in \Cref{eq:660:327} to start with $\frac{p^{d-2} \cdot (n/36)}{2^{d-2}} $. If we opted to use another application of the switching lemma with $p=\frac{1}{\log S\cd 36}$ like the other steps instead of the bottom switching step, we would have to start with  $\frac{p}{2} \cd \frac{p^{d-2} \cdot n}{2^{d-2}} = 
\frac{p^{d-1} \cdot (n)}{2^{d-1}}  $ instead (which is a  factor of $\frac{1}{72\log S}$ instead of 1/36), so that overall we would have gotten a weaker lower bound of $2^{\Omega(\sqrt[d]n)}$ instead of the stronger lower bound of $2^{\Omega(\sqrt[d-1]n)}$ .
\end{note}



\section{Proof of the Switching Lemma}

Let \( 0 < p < \frac{1}{9} \), and recall that  \(\mathcal{R}\) is a distribution of partial restrictions
\(\rho\) to the variables \( \bar{x} \) (where \( |\bar{x}| = n \)) as follows:
\[
\Pr[\rho(x_i) = *] = p
\]
\[
\Pr[\rho(x_i) = 0] = \Pr[\rho(x_i) = 1] = \frac{1 - p}{2}
\]
We make the following two assumptions.

Function Assumption: From now on, let \( f \) be an $r$-DNF with \( n \) variables.

Order Assumption:  We assume a fixed order on conjuncts in \( f \),
and a fixed order on literals in each conjunct.

\subsection{The Canonical Decision Tree \( T(f|_{\rho}) \)}

A canonical decision tree for \( f|_{\rho} \) is defined as follows:

\begin{enumerate}
    \item Look through \( f \) for a conjunct \( C \) such that \( C|_{\rho} \neq 0 \).
    \begin{itemize}
        \item If no such \( C \) exists, output "0" (i.e., the tree is the single node \( 0 \)).
        \item Otherwise, let \( C_1 \) be the first such conjunct.
    \end{itemize}
    
    \item Let \( \beta_1 \) be the variables that appear starred in \( C_1|_{\rho} \) (i.e., they are unassigned).
    
    Query all variables in \( \beta_1 \) in order. Denote by \( \pi \) the assignment/path given to these variables.
    
    \item If \( C_1|_{\rho\pi} = 1 \), output "1" (i.e., a node \( 1 \) in this leaf).
    \begin{itemize}
        \item If \( \beta_1 \) was empty (meaning \( C_1|_{\rho} = 1 \)), output "1" as well.
    \end{itemize}
    
    \item Otherwise, \( C_1|_{\rho\pi} = 0 \) and go to step (1), starting with \( \rho\pi \) in place of \( \rho \), looking at conjunct \( C_2 \), which is the first one such that \( C_2|_{\rho\pi} \neq 0 \).
    
    \item Continue in this manner until you run out of conjuncts.
\end{enumerate}

\subsection*{Running example}

\[
F: \quad r\text{-DNF} \quad (x_1 \land x_2 \land x_4 \land \bar{x}_3) \lor (x_5 \land \bar{x}_2 \land x_4) \lor \dots
\]

\[
\rho: \quad 
\begin{array}{ccccc}
x_1 & x_2 & x_3 & x_4 & x_5 \\
1 & * & * & 1 & *
\end{array}
\]

\[
F|_{\rho} = (x_2 \land \bar{x}_3) \lor (x_5 \land \bar{x}_2) \lor \dots
\]

\noindent
Start with the first (from left) conjuncts in \( F|_{\rho} \) that is not \( 0 \). In our case it's
\[
C_1|_{\rho} = (x_2 \land \bar{x}_3).
\]

\noindent
Build query tree for \( C_1|_{\rho} \); i.e., query each variable in order they appear in \( C_1|_{\rho} \):
 
 
 \includegraphics[width=0.3\textwidth]{images/1st-ex-dt.png}
 
 
\noindent
Note that the red path \((x_2 \gets 1, x_3 \gets 0)\) determines an assignment \( \pi_1 \) (for \( x_2, x_3 \)) that satisfies \( 1 \), and hence satisfies \( F|_{\rho} \). Thus, we label the leaf of this path by \( 1 \) (and finish the path there, with no more queries).


\noindent 



\includegraphics[width=0.3\textwidth]{images/ex2-sl-dt.png}

\noindent
Consider each leaf in the tree above. It determines a path \( \pi_1 \) from the root to the leaf.

\noindent
Now, in each such leaf (except the ones labeled by \( 1 \)), we need to continue querying the variables to determine an \textit{extension} of the assignment \( \pi_1 \), so that this extended assignment will either satisfy or falsify \( F|_{\rho} \).


\includegraphics[width=0.3\textwidth]{images/ex3-sl-dt.png}
\noindent

Consider now the leftmost leaf. Then
\[
\pi_1 = \{ x_2 = 0, x_3 = 0 \}
\]
We continue to build the tree from this leaf.

\noindent
The first (left-most) clause in \( F|_{\rho, \pi_1} \) that is non-zero is
\[
C_2|_{\rho, \pi_1} = x_5
\]
So we query \( x_5 \):

\noindent
\includegraphics[width=0.6\textwidth]{images/ex4-sl-dt.png}
\noindent

The pink path is \( \pi_1 \), the green path is \( \pi_2 \).

\noindent
Continue with \( F|_{\rho, \pi_1, \pi_2} \).

\noindent
Note that each layer in a DT can contain queries related to different clauses.
This can happen for instance if \( \pi_i \) on one node makes 0 the next clause \( C_j \), 
while \( \pi_i \) on a different node in the same layer doesn't nullify \( C_j \):
 
\includegraphics[width=0.2\textwidth]{images/ex5-sl-dt.png}
 

\subsection{Connection between canonical decision trees and CNF/DNFs}



\begin{claim}
Let $g$ be a DNF and suppose the height of the decision tree $T(g)$ is at most $s$.
Then $g$ can be written as an $s$-DNF and also as an $s$-CNF.
\end{claim}

\begin{proof}
\textbf{Case 1: $g$ is an $s$-DNF.}
Let $T(g)$ be a decision tree of height $\le s$ computing $g$.
For every root-to-leaf path $\pi$ that ends in a $1$-leaf, define the term
\[
A_\pi \;:=\; \bigwedge_{x_i \text{ queried on }\pi}
\begin{cases}
x_i & \text{if }\pi(x_i)=1,\\
\neg x_i & \text{if }\pi(x_i)=0.
\end{cases}
\]
This is a conjunction of at most $s$ literals. Define
\[
D \;:=\; \bigvee_{\pi \text{ is a $1$-leaf path}} A_\pi .
\]
Then $D \equiv g$: for every input $a\in\{0,1\}^n$, the computation of $T(g)$ on $a$
follows a unique path $\pi(a)$, and we have $D(a)=1$ iff $\pi(a)$ is a $1$-leaf,
which holds iff $g(a)=1$. Hence $g$ is an $s$-DNF.

\textbf{Case 2: $g$ is an $s$-CNF.}
Apply Step~1 to the function $\neg g$. The same decision tree with leaf labels
flipped computes $\neg g$, and its height is still $\le s$. Therefore $\neg g$ has
an $s$-DNF representation
\[
\neg g \;\equiv\; \bigvee_{j} \bigwedge_{i\in I_j} \ell_i ,
\]
where each conjunction has at most $s$ literals. Negating both sides and using De
Morgan's laws gives
\[
g \;\equiv\; \neg(\neg g)
\;\equiv\; \neg\left(\bigvee_{j} \bigwedge_{i\in I_j} \ell_i\right)
\;\equiv\; \bigwedge_{j}\;\neg\left(\bigwedge_{i\in I_j} \ell_i\right)
\;\equiv\; \bigwedge_{j}\;\bigvee_{i\in I_j} \neg \ell_i .
\]
Each clause $\bigvee_{i\in I_j} \neg \ell_i$ has size at most $s$, and hence $g$ is
an $s$-CNF.
\end{proof}



In other words, a canonical decision tree for \( F \) decides \( \neg F \),
if we consider the assignments on 0-leafs.
Thus, if we turn these ANDs corresponding to these assignments to OR of the negated literals, and AND all these ORs, 
we get a CNF computing \( F \).

Given the  claim above, we can rephrase the Switching Lemma (\Cref{thm:Switching-Lemma}) in the terminology of decision tree height as follows: 

\begin{theorem}[Switching Lemma; Decision tree version]\label{thm:SL-DT-version}
If \( f \) is an \( r \)-DNF with \( n \) variables 
and \( \rho \) is a random restriction as above with 
\( p < \frac{1}{9} \),
then with probability \( \leq (9pr)^s \), the height of \( T(f|_{\rho}) \) 
is bigger than \( s \).
\end{theorem}

This means that \( f|_{\rho} \) \textbf{cannot} be written as an \( s \)-CNF 
with probability at most \( (9pr)^s \).

\subsection*{Discussion of the main idea behind the proof}

Let \( \mathcal{R}_{\rho} \) be the distribution of random restrictions, 
taken with probability \( P_{\rho}[X_i = *] = p \) as before.
Let \( S \) be the set of ``bad'' restrictions \( \rho \in \mathcal{R}_{\rho} \).
That is, restrictions for which \( T(f|_{\rho}) \) has height \( > s \).
We wish to bound from above \( P_{\rho\in \mathcal R}[\rho \in S] \).

A priori, we only have the trivial bound:
\[
P_{\rho \in \mathcal{R}_{\rho}}[\rho \in S] \leq 1,
\]
namely, 100\% of restrictions in \( \mathcal{R}_{\rho} \) are bad.

We show how to reduce this probability by a factor of about \( \left( \frac{1}{4} \right)^s \).
Namely, get:
\[
P_{\rho \in \mathcal{R}_{\rho}}[\rho \in S] \leq \left( \frac{1}{4} \right)^s \cdot (\text{small number}).
\]

But how can we show this?
The idea is to show that the probability of the event of picking a bad restriction 
is bounded from above by the probability of picking a restriction from a \emph{different} 
set of restrictions, denoted \( T \).

For the sake of illustration, let's assume that  \( S \) contained a \emph{single} restriction \( \rho \). So our goal would be easy to achieve: we shall bound the probability of picking $\rho$ by the probability of picking a restriction that is longer by  $s$ 
 restricted variables because we have 
\[
\Pr_{\mathcal{R}}[S] \leq \frac{1}{4^s}\cd \Pr_{\mathcal{R}}[\{\rho\}] \leq \frac{1}{4^s} 
\quad \text{(because trivially \( \Pr_{\mathcal{R}}[\{\rho\}] \leq 1\))}
\]
for every restriction \( \rho \) to \( s \) additional variables.
The first inequality follows by the fact that making a restriction \emph{longer}, namely restricting more variables,  
only \emph{increases} its probability  since fixing a \emph{single} variable 
to \( \{0,1\} \) has probability (recall  $p<1/9)$: 
\[
\frac{1 - p}{2} > \frac{8p}{2} = \frac{4}{9},
\]
while fixing a variable to \( * \), i.e., leaving it free, has probability only \( < \frac{1}{9} \).

Here is a simple example when extending the assignment by one bit. Let $\rho$ be a restriction and assume we extend it by one bit to $\rho\sigma$ (namely, $|\sigma|=1$). We have 
$\Pr_{\mathcal{R}}[\rho] = \frac{2}{1 - p} \cdot p \cdot \Pr_{\mathcal{R}}[\rho\sigma]$, because we can obtain $\rho$ from $\rho\sigma$ and un-choosing the variable  fixed by $\sigma$ (this means dividing by $(1-p)/2$) and instead choosing this variable to be free (this means multiplying by $p$). In other words, we get:  
\[
\Pr_{\mathcal{R}}[\rho] = \frac{2}{1 - p} \cdot p \cdot \Pr_{\mathcal{R}}[\rho\sigma] 
<  \frac{2}{8/9} \cdot \frac{1}{9} \cd\Pr_{\mathcal{R}}
[\rho\sigma] = \frac{1}{4}\cd
 \Pr_{\mathcal{R}}[\rho\sigma]\le 1/4.
\]





\begin{figure}
\centering

\includegraphics{switching-mappin-rho.png}

\textbf{Distribution \( \mathcal{R} \) (both sides are the same distribution)}
\[
\rho \longrightarrow \rho\sigma
\]
\caption{Mapping bad restrictions \( \rho\in S_{\beta \pi\gamma} \) via the mapping $\Theta_1$ (notation will become clearer later) to a restriction \( \rho\sigma \) with bigger probability.
The event (of picking) \( \rho \) is depicted as a smaller size circle than the event (of picking) \( \rho\sigma \),
to represent their respective probabilities (weight or ``mass'' in the probability distribution \( \mathcal{R} \)).}
\end{figure}
 

But we need to do something similar not to merely a single assignment \( \rho \), but rather \emph{all} assignments in  \( S \).
To show that the sum of the probabilities of \emph{all} of the restrictions in \( S \) is at most 
\(\frac{1}{4^s} \cdot (\text{small number})\), we can try showing that \emph{each} \( \rho \in S \) can be mapped to a 
\emph{distinct} restriction \( \rho_0 \) as above. 

If the mapping, denoted \( \Theta \), is done to \emph{distinct} restrictions, that is, the mapping is 
\emph{one-to-one} (\emph{``injective''}), then we know that the sum of all probabilities of picking bad restrictions, 
namely the event of picking a restriction from \( S \), is mapped to the \emph{event} of picking a restriction in the 
\emph{image} of \( \Theta \); and
because every event has probability smaller than 1 we finish. 

We explain this in more detail as follows. We have the following 
\[
\Pr_{\mathcal{R}}[S] = \sum_{\rho \in S} \Pr_{\mathcal{R}}[\rho] = 
\sum_{\rho \in S} \left(\frac{1}{4^s} \cdot \Pr_{\mathcal{R}}[\Theta(\rho)]\right) =
\frac{1}{4^s} \cdot \sum_{\rho \in S} \Pr_{\mathcal{R}}[\Theta(\rho)].
\]

Now, we would  like to claim that 
\[
\sum_{\rho \in S} \Pr_{\mathcal{R}}[\Theta(\rho)] \leq 1
\]
and finish. But this is not necessarily true for every sum of probabilities: if the map \( \Theta \) is 
not one-to-one we may end up adding an event with a very large probability more than once, for example, 
hence exceeding one.  

 %But we need to do something similar not to merely a single assignment \( \rho \), but rather all \( S' \).
%To show that the sum of the probabilities of \emph{all} of the restrictions in \( S' \) is at most \(\frac{1}{4^s} \cdot (\text{small number})\), we can try showing that \emph{each} \( \rho \in S' \) can be mapped to a \emph{distinct} restriction \( \rho_0 \) as above. 

%If the mapping, denoted \( \Theta \), is done to \emph{distinct} restrictions, that is, the mapping is \emph{one-to-one} (\emph{``injective''}), then we know that the sum of all probabilities of picking bad restrictions, namely the event of picking a restriction from \( S' \), is mapped to the event of picking a restriction in the \emph{image} of \( \Theta \).

%But because every event has probability smaller than one we finish. We explain it as follows:
%\[
%\Pr_{\mathcal{R}}[S'] = \sum_{\rho \in S'} \Pr_{\mathcal{R}}[\rho] %= 
%\sum_{\rho \in S'} \left(\frac{1}{4^s} \cdot \Pr_{\mathcal{R}}[\Theta(\rho)]\right) %=
%\frac{1}{4^s} \cdot \sum_{\rho \in S'} \Pr_{\mathcal{R}}[\Theta(\rho)].
%\]

%Now, we'd like to claim that 
%\[
%\sum_{\rho \in S'} \Pr_{\mathcal{R}}[\Theta(\rho)] %\leq 1
%\]
%and finish. But this is not necessarily true for %every sum of probabilities: if the map \( \Theta \) is not one-to-one we may end up adding an event with a very large probability more than once, for example, hence exceeding one. 

The way to ensure
$\sum_{\rho \in S} \Pr_{\mathcal{R}}[\Theta(\rho)] \leq 1
$
is by making sure \( \Theta \) is one-to-one, 
hence we do not count probabilities more than once.
In other words, picking restrictions from \(\text{Image}(\Theta(S))\) is an \emph{event} of our probability distribution, and so an event has always probability at most one.

\subsection{Constructing the one-to-one map}


\begin{tcolorbox}[colframe=white, colback=blue!4, boxrule=0mm, sharp corners]
\textbf{The upshot of argument}
\begin{enumerate}
    \item Our goal is to show there is a \emph{one-to-one} mapping from bad restrictions \( \rho \in S \) to some set of  \emph{longer} restrictions \( \rho\sigma\in S' \). 
A \emph{bad} restriction here means a restriction $\rho$ for which \( T(f{\rst}\rho) \) has depth \( > s \). 
    \item If we achieve such a mapping, as explained above, we can conclude that the event of picking a bad restriction is small enough, and roughly bounded from above: \( \Pr_{\mathcal{R}}[\rho \in S ] \leq \frac{1}{4^s} \cdot \Pr_{\mathcal{R}}[ \rho \in S' ] \le \frac{1}{4^s}\), which will be sufficient for us.   
    \item The main challenge is showing the mapping is \emph{one-to-one}. For this purpose,  we roughly build a map that codes bad restrictions \( \rho \in S \) by longer restrictions \( \rho\sigma \), such that given \( \rho\sigma \), we can completely \emph{recover} the restriction \( \rho \) whose code is \( \rho\sigma \) (hence, the map is one-to-one, because distinct restrictions must be mapped to distinct codes).
    \smallskip 
    
     \textbf{Note:} \emph{A prior}, if we know \( \rho\sigma \), it does not mean we can recover \( \rho \), because a different \( \rho' \) can be mapped to \( \rho'\sigma' = \rho\sigma \) (because the \emph{ domains} of the 0-1 assignment in \( \rho \) and \( \rho' \), as well as the domain of   $\sigma$ and $\sigma'$ may be different).
    
    \item Formally, to be able to recover the source restriction $\rho$ from $\rho\sigma$ we shall encode  \( \rho \) by \( \rho\sigma\) \emph{plus some small helper code.} Since these codes will be small, they will not damage our upper bound. Informally, one can think about it as showing that the mapping $\rho$ to $\rho\sigma$ is one-to-few (e.g., if the helper code is one bit, we get a one-to-two mapping, which will still be enough to conclude the upper bound on the probability of picking a bad restriction). 
    
    \item  \label{it:recover-code-SL-upshot}
    The code for $\rho\sigma$  and its helper code are built based on the following  idea: consider a bad restriction \( \rho \in S \), and let \( \pi \) be the first (leftmost) path in \( T(f{\rst}\rho) \) exceeding length \( s \). 
        (We shall prune this path to length \( s \) precisely.)
Then, given only \( \rho\sigma \) and the helper code, we will be able to recover which variables were assigned in \( \rho\sigma \) by this path \( \pi \).     
 This will mean that we are able to recover which variables were assigned by $\sigma$ (the variables assigned by $\pi$ and $\sigma$ are the same). Which in turn, means we are able to know which variables are assigned by $\rho$ (note that variables assigned by $\rho$ cannot be then re-assigned along $\pi$ in the tree $T(f{\rst}\rho)$). Given $\rho\sigma$ and knowing which variable was assigned by either $\rho$ or $\sigma$ allows us to recover $\rho$.  
\end{enumerate}
\end{tcolorbox}

\subsection{Proving the Switching Lemma}
We now have the sufficient concepts to formally prove the switching lemma. 
 
Let \( \rho \in S \) be a bad restriction, namely a restriction under which the canonical decision tree height of $f{\rst_\rho}$ is more than $s$.  
Let \( \pi \) be the \emph{first} path in \( T(f{\rst_{\rho}}) \) with length \( > s \) (this is based on the orders we fixed for terms in the DNF and the order of literals in each AND).




\subsubsection{Running example (continued)}\label{sec:running-example}
We shall continue with our $r$-DNF example. We will see how to encode, and then decode back, bad restrictions $\rho\in S$ based on this example.  Recall the DNF $f$: 
\[
f = (x_1 \wedge x_2 \wedge x_4 \wedge \bar{x}_3) \vee (x_5 \wedge \bar{x}_2 \wedge x_4) \vee (\bar{x}_3 \wedge x_4 x_7) \vee (x_6 \wedge \bar{x}_5) \vee \dots
\]
Let $\rho$ be the following restriction, that we shall assume is a bad one: 
\[
\rho \in S:\qquad
\begin{array}{c|ccccccc}
  \text{variables}~~     & x_1 & x_2 & x_3 & x_4 & x_5 & x_6 & x_7\\ \hline
\text{value of }  \rho  & 1   & *   & *   & 1   & *   & *   & *
\end{array}
\]
When applying $\rho$ to $f$ we get:

\[
f{\rst}_{\rho} = (x_2 \wedge \bar{x}_3) \vee (x_5 \wedge \bar{x}_2) \vee (x_6 \wedge x_5) \vee \dots
\]
Consider the canonical tree representation \(T(f{\rst}_{\rho})\), depicted in the picture below:
\includegraphics{canonical-tree-of-rho-example.png}

Consider the first path (to the left) denoted $\pi$ with length bigger than $s$. This leftmost ``first''  path---witnessing that $\rho$ is a bad restriction---is  marked in yellow. We first trim it to length \emph{precisely}  $s$.
We are going to \emph{encode} $\rho$ with the help of this path. On the other hand, given the code for $\rho$ we shall \emph{not} need to know this path or in general the tree \(T(f{\rst}_{\rho})\). We will be able to \emph{recover} (namely, decode) the restriction $\rho$ from its code, using \emph{only} the code (and knowing $f$)  (see \Cref{it:recover-code-SL-upshot} above).  




\subsubsection{Coding $\rho$}
Look at  \Cref{fig:example-dec-tree-encoding-bad-assignments}, which shows a coding procedure for the canonical decision tree above: 

\begin{figure}[H]
\includegraphics[width=\textwidth]{images/coding-sl-ex1-a.png}
\caption{Example of coding a bad restriction by looking at the left-most path whose length exceeds $s$ in $T(f|\rho)$. 
%Note: On the same depth (but different paths) in the tree, different variables can be queried. Here, at depth 3, we query \( x_7 \) and \( x_6 \) at different paths.%
}
\label{fig:example-dec-tree-encoding-bad-assignments}
\end{figure}
  
To explain this diagram  and the coding procedure in general we use the following notation:
\begin{itemize}
    \item \( C'_1, \dots, C'_k \) : the conjuncts (ANDs, or terms) in $f{\rst}_\rho$ from which the variables are queried along the yellow path, in order of terms (from left to right, based on the fixed ordering we assumed). Note that 
$$C'_i = C_j{\rst}_{\rho\pi_1\dots\pi_{i-1}},$$
 for some $j$ not necessarily equal to $i$ (this is because some terms $C_j$ become zero along the path, namely under $\rho\pi_1\dots\pi_{\ell}$ (some $\ell \le k$), so there is no variable to query in this term, and we need to skip to the next nonzero term in $f{\rst}_{\rho\pi_1\dots\pi_{\ell}}$). 
    \item \( \beta_1, \dots, \beta_k \) : the tuples of variables that we queried in each term $C'_1,\dots,C'_k$  along the yellow path, in order of appearance (based on the fixed order of clauses, and then the fixed order of literals).
  
    \item \( \pi_1, \dots, \pi_k \) : assignments to \( \beta_i \) according to the sub-assignments of $\pi$, denoted \( \pi_i \), for $i\in[k]$ (i.e.,  $ \pi=\pi_1 \dots \pi_k $).
Recall that we identify paths with the assignments they determine.

    \item \( \sigma_1, \dots, \sigma_k \) : unique assignments to \( \beta_i \) that \emph{satisfy}  \( C_i |_{\rho, \pi_1, \dots, \pi_{i-1}} \), for each $i\in[k]$.
\end{itemize}

\begin{tcolorbox}[colframe=white, colback=blue!4, boxrule=0mm, sharp corners]
The code of $\rho$ consists of the following: the assignment $\rho\sigma$ (where $\sigma$ is defined as above), together with the helper codes $\pi'_1,\dots,\pi'_k $, and the tuples $\beta'_1,\dots,\beta'_k$, as described in what follows.
\end{tcolorbox}


\para{Helper Codes}
\begin{itemize}
    \item \( \beta_i \) is coded by string \( \beta_i' \), of \( |\beta_i| \) numbers, each \( \leq 2r \) (recording for each variable in \( \beta_i \): its location in \( C'_i \) (number \( \leq r \)), and whether it is the last variable in \( \beta_i \) (one bit).
We set  \( \beta' := \beta_1' \cdots \beta_k' \), noting  that the number of possible codes $\beta'$ is at most $ (2r)^s $.
    \item The corresponding assignments \( \pi_i \) are encoded by the values $\pi'_i$ along the path $\pi_i$ in their order of appearance from top to bottom (namely, we record only the \emph{values} each variable in the assignment gets while not recording the variables assigned themselves---as we show they can be recovered).
We thus have  \( \pi' := \pi_1' \pi_2' \cdots \pi_k' \), where the number of possible codes for \( \pi' \) is \( 2^s \). 
\end{itemize}



\noindent
\textbf{The Coding Map} \( \Theta \) is a function:
\[
\Theta: S \to \mathcal{R} \times (2r)^s \times 2^s
\]
defined by:
\[
\Theta: \rho \mapsto (\rho\sigma, \beta', \pi')
\]

\begin{note}
As mentioned above, given \( \rho\sigma \), we do not know a priori how to find \( \rho \) because we do not necessarily  know what is the domain of \( \rho \) and what is the domain of \( \rho\sigma \). We only know the domain of \( \rho\sigma \).
\end{note}



% 
% 
% \noindent\text{Coding Table}
% \begin{center}
% \begin{tabular}{|c|c|c|c|}
% \hline
% \( C_i \)'s ({\tiny Conjuncts on yellow path; these are not in the code)}) & \( \beta_i \) (Unset variables) & \( \pi_i \) (Partial \( \pi \) Assignment) & \( \sigma_i \) \\ 
% \hline
% \( C_1 \) & \( \beta_1 = \{x_2, x_3\} \) & \( \pi_1 = 00 \) & \( \sigma_1 = 10 \) \\
%          & \( \beta_1' = \{\ceil{2}, \ceil{4}\} \) & & \( \Rightarrow \) satisfies \( C_1 \) \\
% \hline
% \( C_2 \) & \( \beta_2 = \{x_5\} \) & \( \pi_2 = 1 \) & \( \sigma_2 = 0 \) \\
%          & \( \beta_2' = \{\ceil{1}\} \) & & \\
% \hline
% \( C_3 \) & \( \beta_3 = \{x_7\} \) & \( \pi_3 = 1 \) & \( \sigma_3 = 1 \) \\
%          & \( \beta_3' = \{\ceil{2}\} \) & & \\
% \hline
% \end{tabular}
% \end{center}

 
\subsubsection{The coding map is one-to-one}   

\begin{lemma}\label{lem:SL-coding-map-1-1}
\(\Theta\) is one-to-one. Namely, given \( (\rho\sigma, \beta', \pi') \) we can \emph{recover}  \( \rho \).
\end{lemma}

\begin{proof}

Recall \( C'_1, C'_2, \dots \) are the clauses we query along the left-most path of length \( >s \) in \( T(f{\rst}_{\rho}) \).
Suppose we are given \( (\rho\sigma, \beta', \pi') \). We can recover \( \rho \) as follows:

\begin{itemize}
    \item First, we can easily recover all strings \( \beta_i' \) and \( \pi_i' \).
    \item Now let \( C_1'' \) be the first term in \( f \) such that \( C'_1|\rho\sigma \neq 0 \).
    \item \( C''_1 \) cannot come before \( C'_1 \), and by construction \( C'_1 \) is not falsified by \( \rho\sigma \).
    \item So we must have \( C_1'' = C'_1 \).
    \item From \( \beta_1' \) and \( C'_1 \) we can recover \( \beta_1 \), and from this and \( \pi_1' \) we can recover \( \pi_1 \).
    \item \( \sigma_1 \) and \( \pi_1 \) were assignments to the same variables, so we can construct a restriction \( \rho\sigma_1[\pi_1/\sigma_1] = \rho\pi_1\sigma_2 \dots \sigma_k \).
    \item Let \( C_2'' \) be the first term in \( f \) such that \( C_2'' | \rho\pi_1\sigma_2 \dots \sigma_k \neq 0 \).
    \item Then as above, \( {C''_2} \) must equal \( C'_2 \), and we can recover \( \beta_2 \) and \( \pi_2 \) and carry on in the same way.

\end{itemize}
 
Once we have recovered all the \( \beta_i \)'s, we know exactly what changed between \( \rho \) and \( \rho\sigma \) and can recover \( \rho \).
\end{proof} 

\begin{note}
\( \rho\sigma \) is not necessarily a bad restriction.
\end{note}
\begin{note}[defining $\sigma$ to be consistent with the conjuncts in f]
\noindent
The point is that we want to avoid a situation in which \( C_i | \rho \not\equiv 0 \) but \( C_i | \rho\sigma \equiv 0 \). If this happens, then we can't know what was the clause queried by \( \pi_i \); it might have been \( C_i \), but because we ask for the first term in \( F \) such that \( C_i' | \rho\sigma \neq 0 \), we don't get to \( C_i \).
\end{note}

\subsection*{Example: Decoding our running example}
\noindent
\( \rho \) is mapped to \( \rho{\sigma} \) with helper codes \( \beta_1', \beta_2', \beta_3', \dots \), \( \pi_1', \pi_2', \pi_3', \dots \).

\[
\rho{\sigma} : x_1, x_2, x_3, x_4, x_5, x_6, \dots
\]

\[
1 \quad 1 \quad 0 \quad 1 \quad 0 \quad * \quad 1
\]

\noindent
We want to recover \( \rho \) given \( \rho{\sigma} \) and helper codes.

\begin{itemize}
    \item Consider the first AND clause \( C''_1 \) in \( f | \rho{\sigma} \) such that \( C_1'' | \rho{\sigma} \neq 0 \).
    \item Since \( \sigma \) never falsifies an AND in \( f \) by \textbf{construction}, then \( C_1'' = C'_1 \) (i.e., the first AND such that \( f | \rho \neq 0 \)).
    \item \( C''_1 \) cannot come before \( C'_1 \) (in \( f \)) because \( C_1'' | \rho{\sigma} \neq 0 \Rightarrow C_1'' | \rho \neq 0 \), since \( \rho{\sigma} \) extends \( \rho \).
    \item Thus, we know that \( C'_1 \) was the first queried AND when we constructed \( T(f{\rst}_ {\rho\sigma}) \).
    \item We know \( \beta'_1 = \{ 2, 4 \} \). So we can find out the variables in $\beta_1$  are \( x_2, x_3 \). Hence, we also know \( \pi_1 \).
    \item So we can change \( \rho{\sigma} \): instead of \( \sigma \), assigning \( x_2, x_3 \) assign \( \pi_1 \) to \( x_2, x_3 \). In symbols, we get:
    \[
    \rho{\sigma} [\sigma \to \pi_1].
    \]
    \item Now, we do the same decoding procedure with \( T(f | \rho{\sigma} [\sigma \to \pi_1]) \), which is precisely the decision tree rooted at the third layer of the tree, left-most node (rooted at $x_5$).
    \item Following this process until the end, we know what is:
    \[
    \rho{\sigma} [\sigma_1 \to \pi_1, \dots, \sigma_k \to \pi_k].
    \]
    Since we know all \( \pi_1, \dots, \pi_k \), we now know what was \( \rho \): We take \( \rho{\sigma} \) and assign stars to all variables queried on the path \( \pi \).
\end{itemize}

\subsubsection{Wrapping up  the Proof of the Switching Lemma}


Let \( \Theta_1: \rho \mapsto \rho{\sigma} \) be the projection of \( \Theta: \rho \mapsto (\rho\sigma, \beta', \pi') \) to the first coordinate \(\rho{\sigma}\).
   When we \emph{fix} the helper codes, \(\beta', \pi'\), the mapping \( \Theta_1 \) \emph{is one-to-one}.
This is because, otherwise two distinct restrictions $\rho\neq\rho'$ would map to the same $\rho\sigma$ with the same helper codes, contradicting \Cref{lem:SL-coding-map-1-1}.
% 
% 
% To see this, fix some helper codes \( \beta_1', \pi_1' \) and let \( S_{\beta_1', \pi_1'} \) be all \( \rho \in S \) such that \( \Theta(\rho) = (\rho{\sigma}, \beta', \pi') \) for some \( \sigma \). That is, all bad assignments with helper codes \( \beta_1', \pi_1' \).
% Consider the projection of \( \Theta \) to the first component:
% \[
% \Theta: S \to \mathcal{R} \times (2r)^s \times 2^s \quad \text{(Recall definition)}
% \]
% \[
% \Theta: \rho \mapsto (\rho{\sigma}, \beta', \pi')
% \]
% \[
% \Theta_1: \rho \mapsto \rho{\sigma}
% \]
% By Claim, \( \Theta_1 \circ S_{\beta_1',\pi_1'} \to \mathcal{R} \) is one-to-one.
% 
% \medskip 

We want to bound from above the probability that a \textit{random} restriction in \( S_{\beta', \pi'} \) is chosen.
Namely, upper bound
\[
\Pr_{\mathcal{R}} [\rho \in S_{\beta', \pi'}].
\]

By definition
\[
\Pr_{\mathcal{R}} [\rho \in S_{\beta', \pi'}] = \sum_{\rho \in S_{\beta', \pi'}} \Pr_{\mathcal{R}} [\rho] = \sum_{\rho \in S_{\beta', \pi'}} \left( \frac{2p}{1-p} \right)^s \Pr_{\mathcal{R}} [\Theta_1(\rho)],
\]
%namely, the probability (in \( \mathcal{R} \)) of choosing the restriction \( \Theta_1(\rho) \), which is \( \rho{\sigma} \) for some \( \sigma \) of size \( s \) with helper codes \( \beta_1', \pi_1' \).
%
where the right equality follows  because  \( \rho\sigma \) sets exactly \( s \) variables that were unset in \( \rho \), meaning that (for every restriction $\rho\in S$):
\[
\Pr_{\mathcal{R}} [\rho ] = p^{s} \cdot \left( \frac{2}{1-p} \right)^s \cdot \Pr_{\mathcal{R}} [\rho\sigma]
\]

By factoring out \( \left( \frac{2p}{1 - p} \right)^s \) we get:

\[
\Pr \left[ \rho \in S_{\beta^{'},\pi^{'}} \right] = \left( \frac{2p}{1 - p} \right)^s \cdot \sum_{\rho \in S_{\beta^{'},\pi^{'}}} \Pr_R [\Theta(\rho)].
\]
But \( \sum_{\rho \in S_{\beta^{'},\pi^{'}}} \Pr_R [\Theta(\rho)] \) is a probability of an \emph{event}, namely it is \( \leq 1 \), because when fixing $\beta',\pi'$ the mapping $\Theta$ is one-to-one as mentioned above\footnote{Namely,  for \( \rho \in S_{\beta^{'},\pi^{'}} \), we never \emph{repeat the same restriction}, that is, if \( \rho \neq \rho' \in S_{\beta^{'},\pi^{'}} \), then \( \Theta(\rho) \neq \Theta(\rho') \); as was discussed above; otherwise we may have counted more than once the same restriction in \( \sum_{\rho \in S_{\beta^{'},\pi^{'}}} \Pr_R [\Theta(\rho)] \).}.
Thus,

\[
\Pr \left[ \rho \in S_{\beta^{'},\pi^{'}} \right] \leq \left( \frac{2p}{1 - p} \right)^s.
\]

Finally, \( S \) is the union of the sets \( S_{\beta^{'},\pi^{'}} \) over all possible strings \( \beta^{'},\pi^{'} \):

\[
\Pr \left[ \rho \in S \right] \leq (2r)^s \cdot 2^s \cdot \left( \frac{2p}{1 - p} \right)^s
= \left( \frac{8 p r}{1 - p} \right)^s \leq (9 p r)^s, \quad \text{for } p < \frac{1}{9}.
\]
This concludes the proof of the switching lemma. 


